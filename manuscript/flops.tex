\section{Numerical simulations}
\label{sec:num_simulations}

%We investigated different computational algorithms for inverting gravity disturbances and total-field anomalies. To test the capability of the equivalent-layer technique methods for processing that potential field data %we measure the computational effort by counting the number of floating-point operations (\textit{flops}), such as additions, subtractions, multiplications, and divisions \citep{golub-vanloan2013} for different number of %observation points, ranging from $10,000$ up to $1, 000, 000$. The results generated when using iterative methods are set to $\textit{it} = 50$ for the number of iterations.
 
\subsection{Floating-point operations calculation}

To measure the computational effort of the different algorithms to solve the equivalent layer linear system, a non-hardware dependent method can be useful because allow us to do direct comparison between them. Counting the floating-point operations (\textit{flops}), i.e., additions, subtractions, multiplications and divisions is a good way to quantify the amount of work of a given algorithm \citep{golub-vanloan2013}. For example, the number of \textit{flops} necessary to multiply two vectors $\mathbb{R}^{N}$ is $2N$. A common matrix-vector multiplication with dimension $\mathbb{R}^{N \times N}$ and $\mathbb{R}^{N}$, respectively, is $2N^2$ and a multiplication of two matrices $\mathbb{R}^{N \times N}$ is $2N^3$. Figure \ref{fig:1} shows the total flops count for the different methods presented in this review with a crescent number of data, ranging from $10,000$ to $1,000,000$ for the gravity equivalent layer and figure \ref{fig:2} for magnetic data. 

\subsubsection{Normal equations using Cholesky decomposition}

The equivalent sources can be estimated directly from solving the normal equations \ref{eq:t_data}. In this work we will use the Cholesky decompositions method to calculate the necessary \textit{flops}. In this method it is calculated the lower triangule of $\mathbf{A}^T\mathbf{A}$ ($1/2 N^3$), the Cholesky factor ($1/3 N^3$), a matrix-vector multiplication ($2N^2$) and finally solving the triangular system ($2N^2$), totalizing

\begin{equation}
	f_{classical} = \dfrac{5}{6} N^3 + 4N^2
\label{flops_classical}
\end{equation}

\subsubsection{Window method \citep{leao-silva1989}}

The moving data-window scheme \citep{leao-silva1989} solve $N$ linear systems with much smaller sizes (equation \ref{eq:p_tk0}) in comparison to the original $N \times N$ system. For our results we are considering a data-window of the same size of wich the authors presented in theirs work ($N_w = 49$) and the same number of equivalent sources ($M_w = 225$). Using the Cholesky decomposition with this method the \textit{flops} are

\begin{equation}
	f_{window} = N\dfrac{5}{6} M_w N_w^2 + 4N_w M_w
\label{flops_leao-silva}
\end{equation}

\subsubsection{PEL method \citep{oliveirajr-etal2013}}

The polynomial equivalent layer uses a similiar approach of moving windows from \cite{leao-silva1989}. For this operations calculation (equation \ref{eq:linear_system_pel}) we used a first degree polynomial (two variables) and each window contains $N_s = 1,000$ observed data and $M_s = 1,000$ equivalent sources. Following the steps given in \citep{oliveirajr-etal2013} the total \textit{flops} becomes

\begin{equation}
	f_{pel} = \dfrac{1}{3} H^3 + 2H^2 + 2NM_sH + H^2N + 2HN + 2NP
\label{flops_pel}
\end{equation}
where $H$ is the number of constant coefficients for the first degree polynomial ($P = 3$) times the number of windows  ($P \times N/N_s$).

\subsubsection{Conjugate gradient least square (CGLS)}

The CGLS method is a very stable and fast algorithm for solving linear systems iteratively. Its computational complexity envolves a matrix-vector product outside the loop ($2N^2$), two matrix-vector products inside the loop ($4N^2$) and six vector products inside the loop ($12N$) \citep{aster2018parameter}

\begin{equation}
	f_{cgls} = 2N^2 + it(4N^2 + 12N)
\label{cgls}
\end{equation}

\subsubsection{Wavelet compression method with CGLS \citep{li-oldenburg2010}}

For the wavelet method (equation \ref{eq:linear_system_li_oldenburg}) we have calculated a compression rate of $98\%$ ( $C_r = 0.02$ ) as the authors used in \cite{li-oldenburg2010} and the wavelet transformation requiring $\log_2(N)$ \textit{flops} each (equations \ref{eq:A_li_oldenburg} and \ref{eq:pl_system_li_oldenburg}c), with its inverse also using the same number of operations (equation \ref{eq:p_li_oldenburg}). Combined with the conjugate gradient least square necessary steps and iterations, the number of \textit{flops} are

\begin{equation}
	f_{wavelet} = 2NC_r + 4N\log_2(N) + it(4N\log_2(N) + 4NC_r + 12C_r)
\label{wavelet}
\end{equation}

\subsubsection{Fast equivalent layer for gravity data \citep{siqueira-etal2017}}

The fast equivalent layer from \cite{siqueira-etal2017} solves the linear system in \textit{it} iterations. The main cost of this method (equations \ref{eq:linear_system_siqueira},\ref{eq:r_siqueira}, \ref{eq:deltap_siqueira} and \ref{eq:p_siqueira}) is the matrix-vector multiplication to asses the predicted data ($2N^2$) and three simply element by element vector sum, subtraction and division ($3N$ total)

\begin{equation}
	f_{siqueira} = it(3N +2N^2)
	\label{siqueira}
\end{equation}

\subsubsection{Convolutional equivalent layer for gravity data \citep{takahashi2020}}

This methods replaces the matrix-vector multiplication of the iterative fast-equivalent technique \citep{siqueira-etal2017} by three steps, involving a Fourier transform, an inverse Fourier transform, and a Hadamard product of matrices (equation \ref{eq:DFT-system_takahashi}). Considering that the first column of our BCCB matrix has $4N$ elements, the flops count of this method is

\begin{equation}
	f_{convgrav} = \kappa4N\log_2(4N) + it(27N + \kappa8N\log_2(4N))
\label{convgrav}
\end{equation}

In the resultant count we considered a \textit{radix-2} algorithm for the fast Fourier transform and its inverse, which has a $\kappa$ equals to 5 and requires $\kappa4N\log_2(4N)$ flops each. The Hadarmard product of two matrices of $4N$ elements with complex numbers takes $24N$ flops. Note that equation \ref{convgrav} is different from the one presented in \cite{takahashi2020} because we also added the flops necessary to calculate the  eigenvalues in this form. It does not differentiate much in order of magnitude because the iterative part is the most costful.

\subsubsection{Convolutional equivalent layer for magnetic data \citep{takahashi2022}}

The convolutional equivalent layer for magnetic data uses the same flops count of the main operations as in the gravimetric case (equation \ref{eq:DFT-system_takahashi}), the difference is the use of the conjugate gradient algorithm to solve the inverse problem.
It requires a Hadamard product outside of the iterative loop and the matrix-vector and vector-vector multiplications inside the loop as seem in equation \ref{cgls}.

\begin{equation}
	f_{convmag} = \kappa16N\log_2(4N) + 24N + it(\kappa16N\log_2(4N) + 60N)
\label{convmag}
\end{equation}

\subsubsection{Deconvolutional method}

The deconvolution method does not require an iterative algorithm, rather it solves the estimative of the physical properties in a single step using the $4N$ eigenvalues of the BCCB matrix as in the convolutional method. From equation \ref{eq:deconvolution_takahashi} it is possible to deduce this method requires two fast Fourier transform ($\kappa4N\log_2(4N)$), one for the eigenvalues and another for the data transformation, an element by element division ($24N$) and finally, a fast inverse Fourier transform for the final estimative ($\kappa4N\log_2(4N)$).

\begin{equation}
	f_{deconv} = \kappa12N\log_2(4N) + 24N
	\label{deconv}
\end{equation}

Using the deconvolutional method with a Wiener stabilization adds two multiplications of complex elements of the conjugates eigenvalues ($24N$ each) and the sum of $4N$ elements with the stabilization parameter $\mu$ as shown in equation \ref{eq:wiener_takahashi}

\begin{equation}
	f_{deconvwiener} = \kappa12N\log_2(4N) + 76N
	\label{deconvwiener}
\end{equation}
