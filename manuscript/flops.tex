\section{Numerical simulations}
\label{sec:num_simulations}

%We investigated different computational algorithms for inverting gravity disturbances and total-field anomalies. To test the capability of the equivalent-layer technique methods for processing that potential field data %we measure the computational effort by counting the number of floating-point operations (\textit{flops}), such as additions, subtractions, multiplications, and divisions \citep{golub-vanloan2013} for different number of %observation points, ranging from $10,000$ up to $1, 000, 000$. The results generated when using iterative methods are set to $\textit{it} = 50$ for the number of iterations.
 
\subsection{Floating-point operations calculation}

To measure the computational effort of the different algorithms to solve the equivalent layer linear system, a non-hardware dependent method can be useful because allow us to do direct comparison between them. Counting the floating-point operations (\textit{flops}), i.e., additions, subtractions, multiplications and divisions is a good way to quantify the amount of work of a given algorithm \citep{golub-vanloan2013}. For example, the number of \textit{flops} necessary to multiply two vectors $\mathbb{R}^{D}$ is $2D$. A common matrix-vector multiplication with dimension $\mathbb{R}^{D \times D}$ and $\mathbb{R}^{D}$, respectively, is $2D^2$ and a multiplication of two matrices $\mathbb{R}^{D \times D}$ is $2D^3$. Figure \ref{fig:1} shows the total flops count for the different methods presented in this review with a crescent number of data, ranging from $10,000$ to $1,000,000$. 

\subsubsection{Normal equations using Cholesky decomposition}

The equivalent sources can be estimated directly from solving the normal equations \ref{eq:delta-q-tilde}. In this work we will use the Cholesky decompositions method to calculate the necessary \textit{flops} for a overdetermined problem (equation \ref{eq:matrix-B-overdetermined}). In this method it is necessary to calculate the lower triangule matrix of the left side equation ($1/2 D^3$), the Cholesky factor ($1/3 D^3$), a matrix-vector multiplication ($2D^2$) and finally solving the triangular system ($2D^2$), totalizing

\begin{equation}
	f_{classical} = \dfrac{5}{6} D^3 + 4D^2
\label{flops_classical}
\end{equation}

\subsubsection{Window method \citep{leao-silva1989}}

The moving data-window scheme \citep{leao-silva1989} solve $M$ linear systems with much smaller sizes (equation \ref{eq:p_tk0}) in comparison to the original $D \times D$ system. For our results we are considering a data-window of the same size of wich the authors presented in theirs work ($D' = 49$) and the same number of equivalent sources ($P' = 225$). Using the algorithm \ref{alg:LS89} as a guide, we have a matrix-matrix multiplication ($2D'^2P'$), a scalar multiplication and a sum with diagonal matrices ($D'$ each), a matrix inverse ($D'^3$), another matrix-matrix product ($2D'P'^2$), a matrix-vetor product ($2P'D'$) and finally a iteration with a vector-vector multiplication ($2D'$). The \textit{flops} are

\begin{equation}
	f_{window} = M2D' + 2P'D' + \dfrac{2D'^{3}}{3} + 2D' + 2D'P'(D'+P')
\label{flops_leao-silva}
\end{equation}

Here we are considering a $2D'^{3}/3$ \textit{flops} count for the Gauss-Jordan inverse matrix algorithm and $M = D$ as we want to calculate the same number of transformations as observation points. Notice that this algorithm takes advantage of a regular grid to calculate only once the inverse matrix and the harmonic functions of $\mathbf{a}'$, with a irregular grid these calculations would be necessary at each iteration. Also this method does not store the equivalent sources estimatives saving computer memory, however, any other potential field transformation would require to run the algorithm again with the compatible harmonic function $\mathbf{a}'$ (equation \ref{eq:harmonic-function-a-kj}).

\subsubsection{PEL method \citep{oliveirajr-etal2013}}

The polynomial equivalent layer uses a similiar approach of moving windows from \cite{leao-silva1989}. For this operations calculation (equation \ref{eq:q-tilde-OBU13}) we used a first degree polynomial (two variables) and each window contains $D' = 1,000$ observed data and $P' = 1,000$ equivalent sources. Following the steps given in \citep{oliveirajr-etal2013} the total \textit{flops} becomes

\begin{equation}
	f_{pel} = \dfrac{1}{3} N_c^3 + 2N_c^2 + 2DP'N_c + N_c^2D + 2N_cD + 2DC
\label{flops_pel}
\end{equation}
where $N_c$ is the number of constant coefficients for the first degree polynomial ($C = 3$) times the number of windows  ($C \times M$).

\subsubsection{Conjugate gradient least square (CGLS)}

The CGLS method is a very stable and fast algorithm for solving linear systems iteratively. Its computational complexity envolves a matrix-vector product outside the loop ($2D^2$), two matrix-vector products inside the loop ($4D^2$) and six vector products inside the loop ($12D$) \citep{aster2018parameter}

\begin{equation}
	f_{cgls} = 2D^2 + it(4D^2 + 12D)
\label{cgls}
\end{equation}

\subsubsection{Wavelet compression method with CGLS \citep{li-oldenburg2010}}

For the wavelet method (equation \ref{eq:linear_system_li_oldenburg}) we have calculated a compression rate of $98\%$ ( $C_r = 0.02$ ) as the authors used in \cite{li-oldenburg2010} and the wavelet transformation requiring $\log_2(D)$ \textit{flops} each (equations \ref{eq:vectors-dw-pw} and \ref{eq:matrix-Gw}), with its inverse also using the same number of operations (equation \ref{eq:vector-p-tilde-LO10}). The normalization using diagonal matrix $\mathbf{L}$ in equations \ref{eq:matrix-GL}, \ref{eq:vector-pL} and \ref{eq:vector-p-tilde-LO10} can be simplified to a matrix-vector product ($2DP$) and two vector-vector products($2P each$). Combined with the conjugate gradient least square necessary steps and iterations, the number of \textit{flops} are

\begin{equation}
	f_{wavelet} = 2DP + 4P + 2DC_r + 4D\log_2(D) + it(4D\log_2(D) + 4DC_r + 12C_r)
\label{wavelet}
\end{equation}

\subsubsection{Fast equivalent layer for gravity data \citep{siqueira-etal2017}}

The fast equivalent layer from \cite{siqueira-etal2017} solves the linear system in \textit{it} iterations. The main cost of this method (algorithm \ref{alg:XS91-SOB17}) is the matrix-vector multiplication to assess the predicted data ($2D^2$) and three simply element by element vector sum, subtraction and division ($3D$ total)

\begin{equation}
	f_{siqueira} = it(3D +2D^2)
	\label{siqueira}
\end{equation}

\subsubsection{Convolutional equivalent layer for gravity data \citep{takahashi2020}}

This methods replaces the matrix-vector multiplication of the iterative fast-equivalent technique \citep{siqueira-etal2017} by three steps, involving a Fourier transform, an inverse Fourier transform, and a Hadamard product of matrices (equation \ref{eq:aux-BCCB-system-diagonalized-3}). Considering that the first column of our BCCB matrix has $4D$ elements, the flops count of this method is a combination of agorithms \ref{alg:XS91-SOB17} and \ref{alg:fast-2D-convolution}

\begin{equation}
	f_{convgrav} = \kappa4D\log_2(4D) + it(27D + \kappa8D\log_2(4D))
\label{convgrav}
\end{equation}

In the resultant count we considered a \textit{radix-2} algorithm for the fast Fourier transform and its inverse, which has a $\kappa$ equals to 5 and requires $\kappa4D\log_2(4D)$ flops each. The Hadarmard product of two matrices of $4D$ elements with complex numbers takes $24D$ flops. Note that equation \ref{convgrav} is different from the one presented in \cite{takahashi2020} because we also added the flops necessary to calculate the  eigenvalues in this form. It does not differentiate much in order of magnitude because the iterative part is the most costful.

\subsubsection{Convolutional equivalent layer for magnetic data \citep{takahashi2022}}

The convolutional equivalent layer for magnetic (algorithm \ref{alg:TOB-2020-2022}) data uses the same flops count of the main operations as in the gravimetric case (equation \ref{eq:aux-BCCB-system-diagonalized-3}), the difference is the use of the conjugate gradient algorithm to solve the inverse problem. It requires a Hadamard product outside of the iterative loop and the matrix-vector and vector-vector multiplications inside the loop as seem in equation \ref{cgls}.

\begin{equation}
	f_{convmag} = \kappa16D\log_2(4D) + 24D + it(\kappa16D\log_2(4D) + 60D)
\label{convmag}
\end{equation}

\subsubsection{Deconvolutional method}

The deconvolution method does not require an iterative algorithm, rather it solves the estimative of the physical properties in a single step using the $4D$ eigenvalues of the BCCB matrix as in the convolutional method. From equation \ref{eq:deconvolution_takahashi} it is possible to deduce this method requires two fast Fourier transform ($\kappa4D\log_2(4D)$), one for the eigenvalues and another for the data transformation, an element by element division ($24D$) and finally, a fast inverse Fourier transform for the final estimative ($\kappa4D\log_2(4D)$).

\begin{equation}
	f_{deconv} = \kappa12D\log_2(4D) + 24D
	\label{deconv}
\end{equation}

Using the deconvolutional method with a Wiener stabilization adds two multiplications of complex elements of the conjugates eigenvalues ($24D$ each) and the sum of $4D$ elements with the stabilization parameter $\mu$ as shown in equation \ref{eq:wiener_takahashi}

\begin{equation}
	f_{deconvwiener} = \kappa12D\log_2(4D) + 76D
	\label{deconvwiener}
\end{equation}
