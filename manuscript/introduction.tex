\section{Introduction}
%%% The begining of the introduction HERE
In accord with potential theory, a continuous potential-field data (gravity and magnetic data) produced by any source can be exactly reproduced by a continuous and infinite 2D physical-property surface distribution that is called the equivalent layer. The equivalent layer is a mathematical solution of Laplace’s equation in the source-free region with the observed potential-field data as the Dirichlet boundary condition \citep{kellogg1929}. Grounded on well-established potential theory, the equivalent-layer technique has been used by exploration geophysicists for processing potential-field data since the late 1960s \citep{dampney1969}. 

Although there was always a great demand for gravity and magnetic data processing, the equivalent-layer technique has not been massively used. This occurs because its high computational cost makes the equivalent-layer technique computationally inefficient for processing massive data sets. In the classic equivalent-layer technique, the continuous problem of the equivalent layer involving integrals is approximated by a discrete form of the equivalent layer. First, a discrete and finite set of equivalent sources (point masses, prisms, magnetic diploes, doublets) is arranged in a layer with finite horizontal dimensions and located below the observation surface. Next, a linear system of equations is set up with a large and full sensitivity matrix. Then, a regularized linear inverse problem is solved to estimate the physical property of each equivalent source within the discrete equivalent layer subject to fitting a discrete set of potential-field observations. 
Finally, the estimated physical-property distribution within the equivalent layer is used to accomplish the desired processing of the potential-field data (e.g., interpolation, upward/downward continuation,  reduction to the pole). The latter step is done by multiplying the matrix of Green’s functions associated with the desired transformation by the estimated physical-property distribution. 


Beginning in the late 1980s, the equivalent-layer techniques computationally efficient have arose. 
To our knowledge, the first method towards improving the efficiency  was proposed by \cite{leao-silva1989} who used an overlapping moving-window scheme spanning the data set. The strategy adopted in \cite{leao-silva1989} involves solving several smaller, regularized linear inverse problems instead of one large problem. This strategy uses a small data window and distributes equivalent sources on a small regular grid at a constant depth located below the data surface.
\cite{leao-silva1989} ensure that sources window extends beyond the boundaries of the data window.
For each position of the data window, this scheme consists in computing the processed field 
at the center of the data window only and the next estimates of the processed field are 
obtained by shifting the data window across the entire dataset. Recently, \cite{soler-uieda2021} developed a computational approach to increase the efficiency of the equivalent-layer techinique by combining two strategies. The fisrt one --- the block-averaging source locations --- reduces the model parameters and the second strategy --- the gradient-boosted algorithm --- reduces the size of the linear system to be solved by fitting the equivalent source model iteratively along overlapping windows. Notice that the equivalent-layer strategy of using a moving-window scheme either in \cite{leao-silva1989} or in \cite{soler-uieda2021} is similar to discrete convolution.

In another approach to reduce computational workload of the equivalent-layer technique \cite{mendonca-silva1994} developed an iterative procedure by incorporating one data point at a time and thus selecting a smaller data set. This strategy adopted by \cite{mendonca-silva1994} is known as 'equivalent data concept'. \cite{li-oldenburg2010} transformed the full sensitivity matrix into a sparse one using the compression of the coefficient matrix via wavelet transforms based on the orthonormal compactly supported wavelets. For jointly processing the components of gravity-gradient data using the equivalent-source processing, \cite{barnes-lumley2011} applied the quadtree model discretization to generate a sparse linear system of equations. \cite{davis_li2011} adaptively dicretized the model (quadtree model discretization) based on localized anomalies and used wavelet transforms to reduce, reordered the model parameters (Hilbert space-filling curves) and compressed each row of the sensitivity matrix of the reordered parameter set (wavelet transforms). By using the subspace method, \cite{mendoncca2020} reduced the dimension of the linear system of equations to be solved in the equivalent-layer technique. The subspace bases span the parameter-model space and they are constructed by applying the singular value decomposition to the matrix containing the gridded data. These strategies followed by \cite{li-oldenburg2010}, \cite{barnes-lumley2011}, \cite{davis_li2011} and \cite{mendoncca2020} may be grouped into the strategy of compression approaches to solve large linear system of equations.

Following the strategy of reparametrization of the equivalent layer, \cite{oliveirajr-etal2013} reduced the model parameters by approximating the equivalent-source layer by a piecewise-polynomial function defined on a set of user-defined small equivalent-source windows. The estimated parameters are the polynomial coefficients for each window and they are much smaller than the original number of equivalent sources. \cite{siqueira-etal2017} developed an iterative solution where the sensitivity matrix is transformed into a diagonal matrix with constant terms through the use of the 'excess mass criterion' and of the positive correlation between the observed gravity data and the masses on the equivalent layer. \cite{jirigalatu-ebbing2019} combined the Gauss-fast Fourier transform (FFT) with Landweber's algorithm and proposed a fast equivalent-layer technique for jointly processing two-components of the gravity-gradient data.
The Landweber's algorithm has some similarities with with gradient-descent algorithm. The strategies worked out by \cite{siqueira-etal2017} and \cite{jirigalatu-ebbing2019} avoid calculating the Hessian matrix and solving linear system of equations.

Recently, \citeauthor{takahashi2020} (\citeyear{takahashi2020}, \citeyear{takahashi2022}), developed fast and effective equivalent-layer techniques for processing, respectively,  gravity and magnetic data by modifying the forward modeling to estimate the physical-property distribution  over the layer through a 2D discrete convolution that can be efficiently computed via 2D FFT.
These methods took advantage of the Block-Toeplitz Toeplitz-block (BTTB) structure of the sensitivity matrices, allowing them to be calculated by using only their first column.
In practice, the forward modeling uses a single equivalent source, which significantly reduces the the required RAM memory. \citeauthor{takahashi2020} (\citeyear{takahashi2020}, \citeyear{takahashi2022}) employed the strategy of the convolutional equivalent layer using the concept of  BTTB matrices.

Here, we present a comprehensive review of