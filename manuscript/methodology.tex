%% Methodology
\section{Fundamentals}

%Consider a set of $N$ potential-field observations (gravity or magnetic data) 
%$d^{o}_{i}$ $(x_{i}, y_{i}, z_{i})$, $i =  1, \dots, N $, 
%at the $i$th observation point $(x_{i}, y_{i}, z_{i})$ of a Cartesian 
%coordinate system with $x$-, $y$- and $z$-axis pointing to north, east and down, respectively.
%Physically, the discrete set of potential-field observations is produced by a unknown source distribution in the subsurface.
%Mathematically, it represents a discrete set of a harmonic function.
%
%A standard way to deal with the classical equivalent-layer technique is approximate the observed potential-field data by the predicted data,
%which in turn are produced by a fictitious layer of sources, 
%called equivalent layer.
%The equivalent layer is located below the observation surface, at depth 
%$z_0$ ($z_0 > z_i$), and with finite horizontal dimensions being composed by 
%a finite discrete set of equivalent sources (e.g., point masses, dipoles, or prisms). 
%Mathematically, this approximation can be written in matrix notation as
%\begin{equation}
%	\mathbf{d} = \mathbf{A} \mathbf{p} \: ,
%	\label{eq:predicted-data-vector}
%\end{equation}
%where $\mathbf{d}$ is an $N$-dimensional predicted data vector whose $i$th element, $d_{i}$ $(x_{i}, y_{i}, z_{i})$, $i =  1, \dots, N $, is the 
%predicted potential-field observation,
%$\mathbf{p}$ is an $M$-dimensional parameter vector whose $j$th element $p_{j}$
%can be a physical property of the $j$th equivalent source and $\mathbf{A}$ is the $N \times M$ sensitivity matrix whose  $ij$th element $a_{ij}$ is a  harmonic function.
%
%
%\subsection{Computational strategies}
%
%The classical equivalent-layer technique consists of estimating the parameter vector $\mathbf{p}$ from the $N$-dimensional observed data vector 
%$\mathbf{d^{o}}$ whose $i$th element is defined as the 
%$d^{o}_{i}$ $(x_{i}, y_{i}, z_{i})$, $i =  1, \dots, N $.
%Usually, this estimate can be obtained by a regularized least-squares solution 
%The estimated parameter is stable, fits the observed data and can be used to
%yield a desired linear transformation of the data, such as interpolation, upward (or downward) continuation, reduction to the pole,  joint processing of
%gravity gradient data and more.
%Mathematically, the desired linear transformation of the data can be obtained by
%\begin{equation}
%	\hat{\mathbf{t}} = \mathbf{T} \mathbf{p^{\ast}}\: ,
%	\label{eq:t_data}
%\end{equation}
%where $\hat{\mathbf{t}}$ is an $N$-dimensional transformed data vector,
%$\mathbf{p^{\ast}}$ is an $M$-dimensional estimated parameter vector and
%$\mathbf{T}$ is the $N \times M$ matrix of Green's functions whose  $ij$th element  is the transformed field at the $i$th observation point produced by the $j$th equivalent source.
%
%The biggest hurdle to use the classical equivalent-layer technique 
%is the computational complexity to handle large datasets 
%because the sensitivity matrix  $\mathbf{A}$ 
%(equation \ref{eq:predicted-data-vector}) is dense. 
%Usually, the estimated parameter vector $\mathbf{p^{\ast}}$
%requires to solve a large-scale linear inversion which in turn means to deal with some obstacles concerning large computational cost:
%i)   the large computer memory to store large and full matrices;
%ii)  the long computation time to mutiply a matrix by a vector; and
%iii) the long computation time to solve a large linear system of equations.
%
%
%Here, we review some strategies for reducing the computational cost of equivalent-layer technique.
%These strategies are the following:

$d^{o}_{i}$

$(x_{i}, y_{i}, z_{i})$

$i \in \{1:D\}$

approximate $d^{o}_{i}$ by a harmonic function
\begin{equation}
	d^{p}_{i} = \sum\limits_{j = 1}^{P} g_{ij} \, p_{j} \: ,
	\quad i \in \{1:D\} \: ,
	\label{eq:predicted-data-dp-i}
\end{equation}
where,

$p_{j}$

$(x_{j}, y_{j}, z_{j})$

$j \in \{1:P\}$

\begin{equation}
	g_{ij} \equiv g(x_{i} - x_{j}, y_{i} - y_{j}, z_{i} - z_{j}) \: ,
	\quad z_{i} < \min\{z_{j}\} \: , \quad \forall i \in \{1:D\} \: ,
	\label{eq:harmonic-function-g-ij}
\end{equation}
where $\min\{z_{j}\}$ denotes the minimum $z_{j}$ (or the vertical coordinate of the shallowest equivalent source).

\begin{equation}
	\mathbf{d}_{p} = \mathbf{G} \mathbf{p} \: ,
	\label{eq:predicted-data-vector}
\end{equation}
where $\mathbf{p}$ is a $P \times 1$ vector with $j$-th element $p_{j}$ representing the scalar physical property
of the $j$-th equivalent source 
and $\mathbf{G}$ is a $D \times P$ matrix with element $g_{ij}$ given by equation \ref{eq:harmonic-function-g-ij}. 

The equivalent-layer technique consists in solving a linear inverse problem to determine a parameter vector $\mathbf{p}$ 
leading to a predicted data vector $\mathbf{d}_{p}$ (equation \ref{eq:predicted-data-vector}) \textit{sufficiently close to} the 
observed data vector $\mathbf{d}_{o}$, whose $i$-th element $d^{o}_{i}$ is the observed potential field at $(x_{i}, y_{i}, z_{i})$.
The notion of \textit{closeness} is intrinsically related to the concept of \textit{vector norm} \cite[e.g.,][p. 68]{golub-vanloan2013}
or \textit{measure of length} \cite[e.g.,][p. 41]{menke2018}.
Because of that, almost all methods for determining $\mathbf{p}$ actually estimate a parameter 
vector $\tilde{\mathbf{p}}$ minimizing a length measure of the difference between $\mathbf{d}_{p}$ and $\mathbf{d}_{o}$
(see subsection \ref{subsec:general-formulation}).
Given an estimate $\tilde{\mathbf{p}}$, it is then possible to compute a potential field transformation 
\begin{equation}
	\mathbf{d}_{t} = \mathbf{T} \tilde{\mathbf{p}} \: ,
	\label{eq:transformation}
\end{equation}
where $\mathbf{d}_{t}$ is a $T \times 1$ vector with $k$-th element $d^{t}_{k}$ representing the transformed potential field at
the position $(x_{k}, y_{y}, z_{k})$, $k \in \{1:T\}$, and
\begin{equation}
	t_{kj} \equiv t(x_{k} - x_{j}, y_{k} - y_{j}, z_{k} - z_{j}) \: ,
	\quad z_{k} < \min\{z_{j}\} \: , \quad \forall k \in \{1:T\} \: ,
	\label{eq:harmonic-function-t-kj}
\end{equation}
is a harmonic function representing the $kj$-th element of the $T \times P$ matrix $\mathbf{T}$.

\subsection{Spatial distribution and total number of equivalent sources}
\label{subsec:spatial-distribution-sources}

There is no well-established criteria to define the optimum number $P$ or the spatial distribution
of the equivalent sources. We know that setting an equivalent layer with more (less) sources than potential-field 
data usually leads to an underdetermined (overdetermined) inverse problem \cite[e.g.,][ p. 52--53]{menke2018}.
Concerning the spatial distribution of the equivalent sources, the only condition is that they must rely on a 
surface that is located below and does not cross that containing the potential field data.
\citet{soler-uieda2021} present a practical discussion about this topic.

From a theoretical point of view, the equivalent layer reproducing a given potential field data set cannot cross the
true gravity or magnetic sources. This condition is a consequence of recognizing that the equivalent layer is essentially an indirect solution of 
a boundary value problem of potential theory \citep[e.g.,][]{roy1962,zidarov1965,dampney1969,li_etal_2014,reis-etal2020}.
In practical applications, however, there is no guarantee that this condition is satisfied. 
Actually, its is widely known from practical experience \cite[e.g.,][]{gonzalez-etal2022} that the equivalent-layer technique
works even for the case in which the layer cross the true sources. 

CRITÉRIOS PARA DEFINIR A PROFUNDIDADE DA CAMADA: DAMPNEY (ESPAÇAMENTO DO GRID) E REIS (ESPAÇAMENTO DAS LINHAS)

\subsection{Sensitivity matrix $\mathbf{A}$}
\label{subsec:sensitivity-matrix}

Generally, the harmonic function $g_{ij}$ (equation \ref{eq:harmonic-function-g-ij}) is defined in terms of the 
inverse distance between the observation point $(x_{i}, y_{i}, z_{i})$ and the $j$-th equivalent source at $(x_{j}, y_{j}, z_{j})$,
\begin{equation}
	\frac{1}{r_{ij}} \equiv \frac{1}{\sqrt{(x_{i} - x_{j})^{2} + (y_{i} - y_{j})^{2} + (z_{i} - z_{j})^{2}}} \: ,
	\label{eq:inverse-distance-ij}
\end{equation}
or by its partial derivatives of first and second orders, respectively given by
\begin{equation}
	\partial_{\alpha} \frac{1}{r_{ij}} \equiv \frac{-(\alpha_{i} - \alpha_{j})}{r_{ij}^{3}} \: ,
	\quad \alpha \in \{ x, y, z \} \: ,
	\label{eq:deriv-1-inverse-distance-ij}
\end{equation}
and
\begin{equation}
	\partial_{\alpha\beta} \frac{1}{r_{ij}} \equiv 
	\begin{cases}
		\frac{3 \, (\alpha_{i} - \alpha_{j})^{2}}{r_{ij}^{5}} \: , &\alpha = \beta \: , \\
		\frac{3 \, (\alpha_{i} - \alpha_{j}) \, (\beta_{i} - \beta_{j})}{r_{ij}^{5}} - \frac{1}{r_{ij}^{3}} \: , &\alpha \ne \beta \: , \\
	\end{cases}
	\quad \alpha, \beta \in \{ x, y, z \} \: .
	\label{eq:deriv-2-inverse-distance-ij}
\end{equation}
In this case, the equivalent layer is formed by punctual sources representing monopoles or dipoles
\cite[e.g.,][]{dampney1969, emilia1973, leao-silva1989, cordell1992, oliveirajr-etal2013, siqueira-etal2017, reis-etal2020, takahashi-etal2020, soler-uieda2021, takahashi-etal2022}.
Another common approach consists in not defining $g_{ij}$ by using equations \ref{eq:inverse-distance-ij}--\ref{eq:deriv-2-inverse-distance-ij},
but other harmonic functions obtained by integrating them over the volume of regular prisms 
\cite[e.g.,][]{li-oldenburg_2010, barnes-lumley_2011, li_etal_2014, jirigalatu-ebbing2019}.
There are also some less common approaches defining the harmonic function $g_{ij}$ (equation \ref{eq:harmonic-function-g-ij})
as the potential field due to plane faces with constant physical property \citep{hansen-miyazaki1984}, doublets \citep{silva1986} or
by computing the double integration of the inverse distance function with respect to $z$ \citep{guspi-novara2009}.

A common assumption for most of the equivalent-layer methods is that the harmonic function $g_{ij}$ 
(equation \ref{eq:harmonic-function-g-ij}) is independent on the actual physical relationship between the
observed potential field and their true sources \cite[e.g.,][]{cordell1992, guspi-novara2009,li_etal_2014}.
Hence, $g_{ij}$ can be defined according to the problem.
The only condition imposed to this function is that it decays to zero as the observation point $(x_{i}, y_{i}, z_{i})$
goes away from the position $(x_{j}, y_{j}, z_{j})$ of the $j$-th equivalent source.
However, several methods use a function $g_{ij}$ that preserves the physical relationship between the
observed potential field and their true sources.
For the case in which the observed potential field is gravity data, $g_{ij}$ is commonly defined as a component of 
the gravitational field produced at $(x_{i}, y_{i}, z_{i})$ by a point mass or prism located at $(x_{j}, y_{j}, z_{j})$, with unit density.
On the other hand, $g_{ij}$ is commonly defined as a component of the 
magnetic induction field produced at $(x_{i}, y_{i}, z_{i})$ by a dipole or prism located at $(x_{j}, y_{j}, z_{j})$,
with unit magnetization intensity, when the observed potential field is magnetic data.

For all harmonic functions discussed above, the sensitivity matrix $\mathbf{G}$ (equation \ref{eq:predicted-data-vector}) 
is always dense. For scattered potential-field data, $\mathbf{G}$ does not have a well-defined structure, regardless of
whether the spatial distribution of the equivalent sources is set.
Nevertheless, for the particular case in which (i) there is a single equivalent source right below each potential-field
datum and (ii) both data and sources rely on planar and regularly spaced grids, \citet{takahashi-etal2020,takahashi-etal2022}
show that $\mathbf{G}$ assumes a block-Toeplitz Toeplitz-block (BTTB) structure. In this case, the product of $\mathbf{G}$ and an 
arbitrary vector can be efficiently computed via 2D fast Fourier transform as a discrete convolution.

\subsection{General formulation}
\label{subsec:general-formulation}

A general formulation for almost all equivalent-layer methods can be achieved by first considering that
the $P \times 1$ parameter vector $\mathbf{p}$ (equation \ref{eq:predicted-data-vector}) can be reparameterized 
into a $Q \times 1$ vector $\mathbf{q}$ according to:
\begin{equation}
	\mathbf{p} = \mathbf{H} \, \mathbf{q} \: ,
	\label{eq:reparameterization}
\end{equation}
where $\mathbf{H}$ is a $P \times Q$ matrix.
This reparameterization is usually defined with $Q << P$ to reduce the original number of parameters.
The predicted data vector $\mathbf{d}_{p}$ (equation \ref{eq:predicted-data-vector}) can then be
rewritten as follows:
\begin{equation}
	\mathbf{d}_{p} = \mathbf{G} \, \mathbf{H} \, \mathbf{q} \: .
	\label{eq:predicted-data-vetor-reparameterized}
\end{equation}

Then, the problem of estimating a parameter vector $\tilde{\mathbf{p}}$ minimizing a length 
measure of the difference between $\mathbf{d}_{p}$ (equation \ref{eq:predicted-data-vector}) and $\mathbf{d}_{o}$
is replaced by that of estimating an auxiliary vector $\tilde{\mathbf{q}}$ minimizing the goal function
\begin{equation}
	\Gamma(\mathbf{q}) = \Phi(\mathbf{q}) + \mu \: \Theta(\mathbf{q}) \: ,
	\label{eq:function-Gamma}
\end{equation}
which is a combination of particular measures of length given by
\begin{equation}
	\Phi(\mathbf{q}) = \| \mathbf{W}_{d} \left( \mathbf{d}_{o} - \mathbf{G} \, \mathbf{H} \, \mathbf{q} \right) \|_{2}^{2} \: ,
	\label{eq:function-Phi}
\end{equation}
and
\begin{equation}
	\Theta(\mathbf{q}) = \| \mathbf{W}_{q} \, \left( \mathbf{q} - \mathbf{q}_{a} \right) \|_{2}^{2} \: ,
	\label{eq:function-Theta}
\end{equation}
where $\mu$ is a positive scalar controlling the trade-off between $\Phi(\mathbf{q})$ and $\Theta(\mathbf{q})$; 
$\| \cdot \|_{2}$ is the 2-norm (or Euclidean norm);
$\mathbf{W}_{q}$ is a matrix imposing prior information on $\mathbf{q}$ given by
\begin{equation}
	\mathbf{W}_{q} = \mathbf{W}_{p} \, \mathbf{H} \: ,
	\label{eq:weighting-matrix-q}
\end{equation}
with $\mathbf{W}_{p}$ being a matrix imposing prior information on $\mathbf{p}$;
$\mathbf{q}_{a}$ is a $Q \times 1$ vector of reference values for $\mathbf{q}$ satisfying
\begin{equation}
	\mathbf{p}_{a} = \mathbf{H} \, \mathbf{q}_{a} \: ,
	\label{eq:reparameterization-reference}
\end{equation}
with $\mathbf{p}_{a}$ being a $P \times 1$ vector containing reference values
for the original parameter vector $\mathbf{p}$ and 
$\mathbf{W}_{d}$ is an $D \times D$ matrix defining the relative importance of each observed datum $d^{o}_{i}$.
After obtaining an estimate $\tilde{\mathbf{q}}$ for the reparameterized parameter vector $\mathbf{q}$ (equation \ref{eq:reparameterization}) minimizing
$\Gamma(\mathbf{q})$ (equation \ref{eq:function-Gamma}), the estimate $\tilde{\mathbf{p}}$ for the original parameter vector 
(equation \ref{eq:predicted-data-vector}) is computed by 
\begin{equation}
	\tilde{\mathbf{p}} = \mathbf{H} \, \tilde{\mathbf{q}} \: .
	\label{eq:vector-p-tilde}
\end{equation}

The reparameterized vector $\tilde{\mathbf{q}}$ is obtained by first computing the gradient of $\Gamma(\mathbf{q})$,
\begin{equation}
	\boldsymbol{\nabla} \Gamma(\mathbf{q}) = 
	-2 \, \mathbf{H}^{\top}\mathbf{G}^{\top}\mathbf{W}_{d}^{\top} \mathbf{W}_{d} \left(\mathbf{d}_{o} - \mathbf{d}_{p} \right) +
	2 \, \mu \, \mathbf{H}^{\top}\mathbf{W}_{p}^{\top}\mathbf{W}_{p}\mathbf{H} \left( \mathbf{q} - \mathbf{q}_{a} \right) \: .
	\label{eq:gradient-Gamma}
\end{equation}
Then, by considering that $\boldsymbol{\nabla} \Gamma(\tilde{\mathbf{q}}) = \mathbf{0}$ (equation \ref{eq:gradient-Gamma}),
where $\mathbf{0}$ is a vector of zeros, as well as adding and subtracting the term
$\left( \mathbf{H}^{\top}\mathbf{G}^{\top}\mathbf{W}_{d}^{\top} \mathbf{W}_{d} \mathbf{G} \, \mathbf{H} \right) \mathbf{q}_{a}$,
we obtain
\begin{equation}
	\tilde{\mathbf{q}} = \mathbf{q}_{a} + \mathbf{B} \left( \mathbf{d}_{o} - \mathbf{G} \, \mathbf{H} \, \mathbf{q}_{a} \right) \: ,
	\label{eq:vector-q-tilde}
\end{equation}
where 
\begin{equation}
	\mathbf{B} = \left( \mathbf{H}^{\top} \mathbf{G}^{\top} \mathbf{W}_{d}^{\top} \mathbf{W}_{d} \, \mathbf{G} \, \mathbf{H} + 
	\mu \, \mathbf{H}^{\top}\mathbf{W}_{p}^{\top}\mathbf{W}_{p} \, \mathbf{H} \right)^{-1}
	\mathbf{H}^{\top} \mathbf{G}^{\top} \mathbf{W}_{d}^{\top} \mathbf{W}_{d} \: ,
	\label{eq:matrix-B-overdetermined}
\end{equation}
or, equivalently \cite[][p. 62]{menke2018},
\begin{equation}
	\mathbf{B} = \left( \mathbf{H}^{\top}\mathbf{W}_{p}^{\top}\mathbf{W}_{p} \, \mathbf{H} \right)^{-1}
	\mathbf{H}^{\top} \mathbf{G}^{\top}
	\left[ \mathbf{G} \, \mathbf{H} \left( \mathbf{H}^{\top}\mathbf{W}_{p}^{\top}\mathbf{W}_{p} \, \mathbf{H} \right)^{-1} 
	\mathbf{H}^{\top}\mathbf{G}^{\top} + \mu \left( \mathbf{W}_{d}^{\top} \mathbf{W}_{d} \right)^{-1} \right]^{-1} \: .
	\label{eq:matrix-B-underdetermined}
\end{equation}
Evidently, we have considered that all inverses exist in equations \ref{eq:matrix-B-overdetermined} and \ref{eq:matrix-B-underdetermined}.
Matrix $\mathbf{B}$ defined by equation \ref{eq:matrix-B-overdetermined} is commonly used for the cases in which $D > P$, i.e., when
there are more data $d^{o}_{i}$ than parameters $p_{j}$ (overdetermined problems).
On the other hand, for the cases in which there are more parameters than data (underdetermined problems), matrix $\mathbf{B}$ is 
usually defined according to equation \ref{eq:matrix-B-underdetermined}.

PAREI AQUI

The great majority of equivalent-layer methods use non-null $\mathbf{p}_{r}$, $\mu$, $\mathbf{P}$ and $\mathbf{D}$ 
(equations \ref{eq:vector-p-tilde-general}--\ref{eq:matrix-C-underdetermined}) to regularize the inverse problem,
impose some constraint on the estimated parameter vector $\tilde{\mathbf{p}}$ and define weights for the observed data
according to their (presumed) experimental error.
Nonetheless, we limit our analysis to the cases in which $\mathbf{p}_{r}$ have null elements, 
matrices $\mathbf{D}$ and $\mathbf{P}$ are equal to the identity and
$\mu = 0$ because these terms do not significantly impact the computational performance.
Hence, we consider that the estimate $\tilde{\mathbf{p}}$ for overdetermined problems ($I > J$) is obtained 
by solving the linear system
\begin{equation}
	\left( \mathbf{A}^{\top}\mathbf{A} \right) \tilde{\mathbf{p}} = \mathbf{A}^{\top} \mathbf{d} \: ,
	\label{eq:vector-p-tilde-overdetermined}
\end{equation}
whereas the solution for underdetermined problems ($I < J$) is obtained in two steps by first solving a linear system
and then computing a matrix-vector product as follows
\begin{equation}
	\begin{split}
		\left( \mathbf{A} \, \mathbf{A}^{\top} \right) \mathbf{u} &= \mathbf{d} \\
		\tilde{\mathbf{p}} &= \mathbf{A}^{\top} \mathbf{u}
	\end{split} \: ,
	\label{eq:vector-p-tilde-underdetermined}
\end{equation}
where $\mathbf{u}$ is a dummy vector.

Let us first consider the following generic linear system
\begin{equation}
	\mathbf{H} \mathbf{v} = \mathbf{h} \: ,
	\label{eq:square-linear-system-generic}
\end{equation}
where $\mathbf{H}$ is a square matrix and $\mathbf{v}$ is a vector of unknowns.
Here, we set $\mathbf{H} = \mathbf{A}^{\top}\mathbf{A}$ and $\mathbf{h} = \mathbf{A}^{\top}\mathbf{d}$
for solving the overdetermined system (equation \ref{eq:vector-p-tilde-overdetermined}) or
$\mathbf{H} = \mathbf{A} \, \mathbf{A}^{\top}$ and $\mathbf{h} = \mathbf{d}$
for solving the underdetermined system (equation \ref{eq:vector-p-tilde-underdetermined}).

An estimate $\tilde{\mathbf{v}}$ for the unknown vector $\mathbf{v}$ can be obtained by using direct methods, 
such as LU, Cholesky or QR factorization \cite[][p. 111, 163, 246]{golub-vanloan2013}, or iterative methods, 
such as Gauss-Seidel, Successive Over-Relaxation (SOR) or Conjugate Gradient
\cite[][p. 612, 619, 625]{golub-vanloan2013}, for example.
Here, we briefly discuss the Cholesky factorization and some general aspects of iterative methods.

The Cholesky factorization \cite[][p. 163]{golub-vanloan2013} presumes the existence of an upper triangular matrix $\mathbf{G}$ 
(the Cholesky factor) such that $\mathbf{H} = \mathbf{G} \, \mathbf{G}^{\top}$. 
Given $\mathbf{G}$, an estimate $\tilde{\mathbf{v}}$ for the vector of unknowns $\mathbf{v}$ (equation \ref{eq:square-linear-system-generic}) 
can be obtained by first solving the triangular system 
\begin{equation}
	\mathbf{G} \, \mathbf{w} = \mathbf{h} \: ,
	\label{eq:Cholesky-step-1}
\end{equation}
where $\mathbf{w}$ is a dummy vector, and then computing
\begin{equation}
	\mathbf{G}^{\top} \mathbf{w} = \tilde{\mathbf{v}} \: .
	\label{eq:Cholesky-step-2}
\end{equation}

Iterative methods can be roughly defined as a process that starts with an initial approximation
$\tilde{\mathbf{v}}_{0}$ and then produces ever-better approximations $\tilde{\mathbf{v}}_{\ell}$
for the unknown vector $\mathbf{v}$ (equation \ref{eq:square-linear-system-generic}).
At each iteration, it is necessary to evaluate a predefined convergence criterion (CC).
If the CC is satisfied, the current approximation is considered as 
the solution $\tilde{\mathbf{v}}$ of the linear system and the algorithm stops.
Otherwise, the current approximation $\tilde{\mathbf{v}}_{\ell}$ is used to compute
a new approximation $\tilde{\mathbf{v}}_{\ell+1}$ and the CC is evaluated again. 
A pseudo-code for this generic iterative method can be defined as follows:
\begin{algorithm}
	%\SetAlgoLined
	\Input{}
	$\ell = 0$ \;
	$\tilde{\mathbf{v}}_{\ell} \gets \tilde{\mathbf{v}}_{0}$ \;
	Compute auxiliary variables \;
	\While{CC is not satisfied}{
		Compute $\tilde{\mathbf{v}}_{\ell+1}$ \;
		Compute auxiliary variables \;
		$\tilde{\mathbf{v}}_{\ell} \gets \tilde{\mathbf{v}}_{\ell+1}$ \;
		$\ell \gets \ell + 1$ \;
	}
	$\tilde{\mathbf{v}} \gets \tilde{\mathbf{v}}_{\ell}$
	\caption{Generic pseudo-code for iterative methods.}
\end{algorithm}


\section{Computational strategies}

Two important factors affecting the efficiency of a given matrix algorithm are the 
storage and amount of required arithmetic. Here, we quantify this last factor by counting flops.
A flop is a floating point addition, subtraction, multiplication or division \cite[][ p. 12--14]{golub-vanloan2013}.

NÃO SEI SE TÁ BOM AQUI: To investigate the efficiency of equivalent-layer methods, we consider how they
(i) set up and (ii) solve the linear inverse problem to estimate the physical property distribution on the equivalent layer, 
as well as (iii) perform potential field transformations (equation \ref{eq:transformation}).

We focus on the overall strategies used by the selected methods

\subsection{Moving-data windows}

Split the observed data $d_{i}$, $i \in \{1, \dots, I\}$, into $M$ overlapping subsets (or data windows) 
formed by $I^{m}$ data each, $m \in \{ 1, \dots, M \}$.

The number of data $I^{m}$ forming the data windows are not necessarily equal to each other.

The data forming a given window are usually adjacent to each other.

Each window has an $I^{m} \times 1$ observed data vector $\mathbf{d}^{m}$.

Let each data window be approximated by a local equivalent layer composed of $J^{m}$ sources, so that
its predicted data vector is given by
\begin{equation}
	\mathbf{f}^{m} = \mathbf{A}^{m} \mathbf{p}^{m} \: ,
	\label{eq:predicted-data-subset-m}
\end{equation}
where $\mathbf{p}^{m}$ is a $J^{m} \times 1$ vector containing the scalar physical properties of the 
equivalent sources in the $m$-th subset and
$\mathbf{A}^{m}$ is an $I^{m} \times J^{m}$ matrix whose elements are computed with equation 
\ref{eq:harmonic-function-aij} by using only the coordinates of the observed data and equivalent sources
in the $m$-th subset.

The main advantage of this approach is that the estimated parameter vector $\tilde{\mathbf{p}}$ is not
obtained by solving the full linear system, but several smaller ones.

\cite{leao-silva1989} presented a pioneer work using this approach.

Their method requires a regularly-spaced grid of observed data on a horizontal plane $z_{0}$. 

The data windows are defined by square local grids of $\sqrt{I'} \times \sqrt{I'}$ adjacent points, all of them having the
same number of points $I'$.

The equivalent sources in the $m$-th data window are located below the observation plane, at a constant vertical distance
$\Delta z_{0}$. They are arranged on a regular grid of $\sqrt{J'} \times \sqrt{J'}$ adjacent points 
following the same grid pattern of the observed data. 
The local grid of sources for all data windows have the same number of elements $J'$.
Besides, they are vertically aligned, but expands the limits of their corresponding data windows,
so that $J' > I'$.

Because of this spatial configuration of observed data and equivalent sources, we have that
$\mathbf{A}^{m} = \mathbf{A}'$ (equation \ref{eq:predicted-data-subset-m}) for all data windows, 
where $\mathbf{A}'$ is a constant matrix.

By omitting the regularization and normalization strategies used by \cite{leao-silva1989},
their method consists in combining equations \ref{eq:vector-p-tilde-underdetermined} and 
\ref{eq:transformation} to directly compute the transformed potential field $t^{m}_{c}$ at the central point of
each data window as follows:
\begin{equation}
	t^{m}_{c} = \left( \mathbf{A}' \mathbf{b}' \right)^{\top} \left[ \mathbf{A}' \, \left( \mathbf{A}' \right)^{\top} \right]^{-1} 
	\mathbf{d}^{m} \: , \quad m \in \{ 1, \dots, M \} \: ,
	\label{eq:transformed-field-tmc-LS89}
\end{equation}
where $\mathbf{b}'$ is a $J' \times 1$ vector with elements computed by equation 
\ref{eq:harmonic-function-bkj} by using all equivalent sources in the $m$-th subset and
only the coordinate of the central point in the $m$-th data window. Due to the presumed spatial configuration of the observed
data and equivalent sources, $\mathbf{b}'$ is the same for all data windows.

Their method can be outlined by the following pseudo-code:
\begin{algorithm}
	%\SetAlgoLined
	\Input{}
	Set the $\mathbf{d}^{m}$ for each data window, $m \in \{ 1, \dots, M \}$ \;
	Set the constant depth $z_{0} + \Delta z_{0}$ for all equivalent sources \;
	Compute the vector $\mathbf{b}'$ associated with the desired potential-field transformation \;
	Compute the matrix $\mathbf{A}'$ \;
	Compute $\left( \mathbf{A}' \mathbf{b}' \right)^{\top} \left[ \mathbf{A}' \, \left( \mathbf{A}' \right)^{\top} \right]^{-1}$ \;
	$\ell = 1$ \;
	\While{$\ell < M$}{
		Compute $t^{m}_{c}$ (equation \ref{eq:transformed-field-tmc-LS89}) \;
		$\ell \gets \ell + 1$ \;
	}
	\caption{Generic pseudo-code for the method proposed by \cite{leao-silva1989}.}
	\label{alg:LS89}
\end{algorithm}

Note that \cite{leao-silva1989} directly compute the transformed potential $t^{m}_{c}$ at the central point of
each data window without explicitly computing and storing $\mathbf{p}^{m}$ (equation \ref{eq:predicted-data-subset-m}).
It means that their method allows computing a single potential-field transformation. 
A different transformation or the same one evaluated at different points require running their moving-data window method again.

\cite{soler-uieda2021} generalized the method proposed by \cite{leao-silva1989} for irregularly spaced data on an undulating surface.

The overall steps of their method are defined by the following pseudo-code:
\begin{algorithm}
	%\SetAlgoLined
	\Input{}
	Set the $\mathbf{d}^{m}$ for each data window, $m \in \{ 1, \dots, M \}$ \;
	Set the depth scheme for all equivalent sources \;
	Compute the vector $\mathbf{b}'$ associated with the desired potential-field transformation \;
	Compute the matrix $\mathbf{A}'$ \;
	Compute $\left( \mathbf{A}' \mathbf{b}' \right)^{\top} \left[ \mathbf{A}' \, \left( \mathbf{A}' \right)^{\top} \right]^{-1}$ \;
	$\ell = 1$ \;
	\While{$\ell < M$}{
		Compute $t^{m}_{c}$ (equation \ref{eq:transformed-field-tmc-LS89}) \;
		$\ell \gets \ell + 1$ \;
	}
	\caption{Generic pseudo-code for the method proposed by \cite{soler-uieda2021}.}
	\label{alg:SU21}
\end{algorithm}


PAREI AQUI

\subsection{Column update}

\cite{cordell1992}

\cite{guspi-novara2009}

\subsection{Row update}

Algebraic reconstruction techniques (ART) \cite{sluis-vorst1987}

\cite{mendonca-silva1994}

\subsection{Reparameterization}

\cite{barnes-lumley_2011}

\cite{oliveirajr-etal2013}

\cite{mendonca-2020}

\subsection{Wavelet compression}

\cite{li-oldenburg_2010}

\subsection{Iterative methods using the original $\mathbf{A}$}

\cite{xia-sprowl1991}

\cite{xia-etal1993}

\cite{siqueira-etal2017}

\cite{jirigalatu-ebbing2019}


\subsection{Discrete convolution}

\cite{takahashi-etal2020}

\cite{takahashi-etal2022}

\section{TEXTO ANTIGO}

\cite{leao-silva1989} reduced the total processing time and memory usage
of equivalent-layer technique by means of a moving data-window scheme.
A small moving data window with $N_w$ observations and a small equivalent layer with $M_w$ equivalent sources 
($M_w > N_w$) located  below the observations are established.
For each position of a moving-data window, \cite{leao-silva1989} estimate a stable solution 
$\mathbf{{p_w}^{\ast}}$ by using a data-space approach with the zeroth-order Tikhonov regularization 
\citep{aster2018parameter}, i.e.,
\begin{subequations}
\begin{eqnarray}
\left( \mathbf{A_w} \mathbf{A_w}^{\top} + \mu \mathbf{I}\right) \mathbf{w} &=& \mathbf{d_w}^{o} \:, \\
\mathbf{A_w}^{\top} \: \mathbf{w} &=&  \mathbf{{p_w}^{\ast}} \:, 
\label{eq:p_tk0}
\end{eqnarray}
\end{subequations}
where $\mathbf{w}$ is a dummy vector, 
$\mu$ is a regularizing parameter, 
$\mathbf{d_w}^{o}$ is an $N_w$-dimensional vector containing the observed potential-field data, 
$\mathbf{A_w}$ is an $N_w \times M_w$  sensitivity matrix related to a moving-data window, 
$\mathbf{I}$ is  an identity matrix of order $N_w$ and 
the superscript $\top$ stands for a transpose. 
After estimating an $M_w \times 1$ parameter vector $\mathbf{{p_w}^{\ast}}$  (equation \ref{eq:p_tk0}) 
the desired transformation of the data is only calculated at the central point of each moving-data window, i.e.:
\begin{equation}
	\hat{t}_{k} = \mathbf{t}_k^{\top} \: \mathbf{{p_w}^{\ast}} \: ,
	\label{eq:t_leaosilva}
\end{equation}
where $\hat{t}_{k}$ is the transformed data calculated at the central point ${k}$ of the data window and
$\mathbf{t}_k$ is an $M_w \times 1$ vector whose elements form the $k$th row of the $N_w \times N_w$ 
matrix of Green's functions $\mathbf{T}$ (equation \ref{eq:t_data}) of 
the desired linear transformation of the data.

By shifting the moving-data window with a shift size of one data spacing, 
a new position of a data window is set up.
Next, the aforementioned process (equations \ref{eq:p_tk0} and \ref{eq:t_leaosilva}) is repeated  for each position of a moving-data window, until the entire data have been processed.
Hence, instead of solving a large inverse problem, \cite{leao-silva1989} solve several much smaller ones. 


To reduce the size of the linear system to be solved, \cite{soler-uieda2021}  adopted the same strategy proposed, originally, by \cite{leao-silva1989}  of using a small moving-data window sweeping the whole data.
In \cite{leao-silva1989}, a moving-data window slides to the next adjacent data window following a sequential movement, the predicted data is calculated inside the data window and the desired transformation are only calculated at the center of the moving-data window.
Unlike \cite{leao-silva1989}, \cite{soler-uieda2021} do not adopt a sequential order of the data windows; rather, they adopt a randomized order of windows in the iterations of the gradient-boosting algorithm 
(\citeauthor{friedman2001}, \citeyear{friedman2001} and \citeyear{friedman2002}).
The  gradient-boosting algorithm in \cite{soler-uieda2021} estimates a stable solution using the data and the equivalent sources 
that fall within a moving-data window; however, it calculates the predicted data and the residual data in the whole survey data. 
Next, the residual data  that fall within a new position of the data window is used as input data to estimate a new stable solution within the data window
which in turn is used to calculated a new predicted data and 
a new residual data in the whole survey data.
Finally, unlike \cite{leao-silva1989}, in \cite{soler-uieda2021} neither the data nor the equivalent sources need to be distributed in regular grids.
Indeed, \cite{leao-silva1989} built their method using regular grids, but in fact regular grids are not necessary.
Regarding the equivalent-source layout, \cite{soler-uieda2021} proposed the block-averaged sources locations in which the survey area is divided into horizontal blocks and one single equivalent source is assigned to each block.
Each single source per block is placed over the layer with its horizontal coordinates given by the average horizontal positions of observation points.
According to \cite{soler-uieda2021}, the block-averaged sources layout reduces the number of equivalent sources significantly
and the gradient-boosting algorithm provides even greater efficiency in terms of data fitting.



% PENSAR ONDE COLOCAR OS FLOPS
%In the \cite{leao-silva1989}  equivalent-layer approach, the number of flops  required to solve the linear %system  by Cholesky's decomposition is




\subsubsection{The equivalent-data concept}

To reduced the total processing time and memory usage of equivalent-layer technique, \cite{mendonca-silva1994} proposed a strategy called 'equivalent data concept'.
The equivalent data concept is grounded on the  principle  that there is a subset of redundant data that does not contribute to the final solution and thus can be dispensed.
Conversely, there is a subset of observations, called equivalent data, that  contributes effectively to the final solution and fits the remaining observations (redundant data).
Iteractively, \cite{mendonca-silva1994} selected the subset of equivalent data that is substantially smaller than the original dataset. 
This selection is carried out by incorporating one data point at a time.

According to \cite{mendonca-silva1994}, the number of equivalent data is about one-tenth 
of the total number of observations. 
These authors used the equivalent data concept to carry out an interpolation of gravity data.
They showed a reduction of the total processing time and memory usage by, at least, 
two orders of magnitude as opposed to using all observations in the interpolation process 
via the classical equivalent-layer technique.


\subsubsection{The wavelet compression and lower-dimensional subspace}

For large data sets, the  sensitivity matrix $\mathbf{A}$ (equation \ref{eq:predicted-data-vector}) is a drawback in applying  
the equivalent-layer technique because it is a large and dense matrix.

\cite{li-oldenburg2010} transformed a large and full sensitivity matrix into a sparse one by using fast wavelet transforms.
In the wavelet domain, \cite{li-oldenburg2010} applyied a 2D wavelet transform to each row and column of the original sensitivity matrix $\mathbf{A}$ to expand it in the wavelet bases. 
This operation can be done by premultiplying the original sensitivity matrix $\mathbf{A}$ by a 
matrix representing the 2D wavelet transform $\mathbf{W_2}$ and then the resulting is postmultiplied by 
the transpose of $\mathbf{W_2}$ (i.e., $\mathbf{W_2}^{\top}$).
\begin{equation}
	\mathbf{\tilde{A}} = \mathbf{W_2} \: \mathbf{A}  \: \mathbf{W_2}^{\top} \:,
	\label{eq:A_li_oldenburg}
\end{equation}
where $\mathbf{\tilde{A}}$ is the expanded original sensitivity matrix in the wavelet bases with many elements zero or close to zero.
Next, the matrix $\mathbf{\tilde{A}}$ is replaced by its sparse version $\mathbf{\tilde{A}_{s}}$ 
in the wavelet domain which in turn is obtained by retaining only 
the large elements of the $\mathbf{\tilde{A}}$.
Thus, the elements of  $\mathbf{\tilde{A}}$ whose  amplitudes fall below a relative threshold are discarded.
In \cite{li-oldenburg2010}, the original sensitivity matrix $\mathbf{A}$ is high compressed resulting in 
a sparce matrix $\mathbf{\tilde{A}_{s}}$ with a few percent of nonzero elements and 
the the inverse problem is solved in the wavelet domain by using $\mathbf{\tilde{A}_{s}}$ and 
a incomplete conjugate gradient least squares, without an explicit regularization parameter and 
a limited number of iterations.
The solution is obtained by solving the following linear system
\begin{equation}
	 \mathbf{\tilde{A}_{L}}^{\top} \: \mathbf{\tilde{A}_{L}} \: \mathbf{\tilde{p}_{L}}^{\ast} \: =  
	 \mathbf{\tilde{A}_{L}}^{\top} \: \mathbf{\tilde{d}}^{o} \: ,
	\label{eq:linear_system_li_oldenburg}
\end{equation}
where $\mathbf{\tilde{p}_{L}}^{\ast}$ is obtained by solving the linear system given by equation
\ref{eq:linear_system_li_oldenburg},
\begin{subequations}
\begin{eqnarray}
\mathbf{\tilde{A}_{L}} &=& \mathbf{\tilde{A}_{s}} \: \mathbf{\tilde{L}^{-1}}, \\
\mathbf{\tilde{p}_{L}} &=& \mathbf{\tilde{L}} \mathbf{\tilde{p}}, \\
\mathbf{\tilde{d}}^{o} &=&  \mathbf{W_2} \: \mathbf{d}^{o}, 
\end{eqnarray}
\label{eq:pl_system_li_oldenburg}
\end{subequations}
where $\mathbf{\tilde{L}}$ is a diagonal and invertible weighting matrix representing the finite-difference approximation in the wavelet domain. 
Finally, the distribution over the equivalent layer in the space domain $\mathbf{p}$ is obtained by applying an inverse wavelet transform in two steps, i.e.:
\begin{equation}
	\mathbf{\tilde{p}}  = \mathbf{\tilde{L}}^{-1} \:  \mathbf{\tilde{p}_{L}}^{\ast} \:,
	\label{eq:ptil_li_oldenburg}
\end{equation}
and
\begin{equation}
	 \mathbf{p}  = \mathbf{W_2} \: \mathbf{\tilde{p}} \:.
	\label{eq:p_li_oldenburg}
\end{equation}
Although the data misfit quantifying the difference between the observed and  predicted data 
by the equivalent source is calculated in the wavelet domain, we understand that the desired transformation
is calculated via equation \ref{eq:t_data} which  uses a full matrix of Green's functions $\mathbf{T}$.

\cite{li-oldenburg2010} used the equivalent-layer technique with a wavelet compression to perform 
an  upward continuation of total-field anomaly between uneven surfaces.
For regularly spaced grid of data,  \cite{li-oldenburg2010} reported that high compression ratios 
are achived with insignificant loss of accuracy.
As compared to the upward-continued total-field anomaly by equivalent layer using the dense matrix,
\citeauthor{li-oldenburg2010}'s (\citeyear{li-oldenburg2010}) approach, using the Daubechies wavelet,
decreased CPU (central processing unit) time by up to two orders of magnitude.

\cite{mendoncca2020} overcame the solution of intractable large-scale equivalent-layer problem by using the subspace method (e.g., 
\citeauthor{skilling-bryan1984}, \citeyear{skilling-bryan1984};
\citeauthor{kennett1988}, \citeyear{kennett1988};
\citeauthor{oldenburg1993}, \citeyear{oldenburg1993};  
\citeauthor{barbosa-etal1997}, \citeyear{barbosa-etal1997}).
The subspace method  reduces the dimension of the linear system of equations to be solved. 
Given a higher-dimensional space (e.g., $M$-dimensional model space, 
$\mathbb{R}^{M}$), there exists many lower-dimensional subspaces 
(e.g., $Q$-dimensional subspace) of $\mathbb{R}^{M}$.
The linear inverse problem related to the equivalent-layer technique 
consists in finding an $M$-dimension parameter vector $\mathbf{p} \: \in \mathbb{R}^{M}$ which adequately fits the potential-field data.
The subspace method looks for a parameter vector who lies in a $Q$-dimensional subspace of $\mathbb{R}^{M}$ which, in turn, is spanned by a set of $Q$ vectors 
$\mathbf{v}_i = 1, ..., Q$, where $\mathbf{v}_i \in \mathbb{R}^{M}$
In matrix notation,  the parameter vector in the subspace method 
can be written as  
\begin{equation}
	\mathbf{p} = \mathbf{V} \: \boldmath{\alpha}  \:,
	\label{eq:p_subspace}
\end{equation}
where $\mathbf{V}$ is an $M \times Q$ matrix whose columns 
$\mathbf{v}_i = 1, ..., Q$ form a basis vectors for a subspace $Q$ of 
$\mathbb{R}^{M}$.
In equation \ref{eq:p_subspace}, the parameter vector $\mathbf{p}$ 
is defined as a linear combination in the space spanned by $Q$ basis vectors 
$\mathbf{v}_i = 1, ..., Q$  and {\boldmath$\alpha$}  is a $Q$-dimensional 
unknown vector to be determined.
The main advantage of the subspace method is that the linear system of 
$M$ equations in $M$ unknowns to be originally solved 
is reduced to a new linear system of $Q$ equations in $Q$ unknowns
which requires much less computational effort since $Q << M$, i.e.:
\begin{equation}
	 \mathbf{V}^{\top} \:  \mathbf{A}^{\top} \: \mathbf{A} \:  \mathbf{V}  \: \boldmath{\alpha}^{\ast} \: = \:
	 \mathbf{V}^{\top}  \: \mathbf{d}^{o} \:.
	\label{eq:linear_system_subspace}
\end{equation}
To avoid the storage of matrices $\mathbf{A}$ and $\mathbf{V}$,  \cite{mendoncca2020} 
evaluates an element of the matrix  $\mathbf{A} \mathbf{V}$ by
calculating the dot product between the row of matrix $\mathbf{A}$ and the column of the matrix 
$\mathbf{B}$. 
After estimating {\boldmath$\alpha^{\ast}$} (equation \ref{eq:linear_system_subspace}) 
belonging to a $Q$-dimensional subspace of $\mathbb{R}^{M}$,
the distribution over the equivalent layer  $\mathbf{p}$ in the $\mathbb{R}^{M}$ is obtained by applying equation \ref{eq:p_subspace}.
The choice of the $Q$ basis vectors $\mathbf{v}_i = 1, ..., Q$ 
(equation \ref{eq:p_subspace}) in the subspace method is not strict.
\cite{mendoncca2020}, for example, chose the eigenvectors yielded by 
applying the singular value decomposition of the matrix containing the gridded data set.
The number of eigenvectors used to form basis vectors will depend on 
the singular values. 

The proposed subspace method for solving large-scale equivalent-layer problem by \cite{mendoncca2020} was applied to  estimate the mass excess or deficiency caused by causative gravity sources.


\subsubsection{The quadtree discretization}

To make the equivalent-layer technique tractable, \cite{barnes-lumley2011} also transformed the dense sensitivity matrix $\mathbf{A}$ (equation \ref{eq:predicted-data-vector}) into a sparse matrix.
In \cite{barnes-lumley2011}, a sparce version of the sensitivity matrix is achived by grouping equivalent sources (e.g., they used prisms) distant from an observation point together to form a larger prism 
or larger block.
Each larger block has averaged physical properties and averaged top- and bottom-surfaces of the grouped smaller prisms (equivalent sources) that are encompassed by the larger block.
The authors called it the 'larger averaged block' and the essence of their method is the reduction 
in the number of equivalent sources, which means a reduction in the number of parameters to be estimated 
implying in model dimension reduction.

The key of the \citeauthor{barnes-lumley2011}'s (\citeyear{barnes-lumley2011}) method is the 
algorithm for deciding how to group the smaller prisms.
In practice, these authors used a recursive bisection process that results in a quadtree discretization  
of the equivalent-layer model. 

By using the quadtree discretization, \cite{barnes-lumley2011} were able to jointly process multiple components of airborne gravity-gradient data using a single layer of equivalent sources. 
To our knowledge, \cite{barnes-lumley2011} are the pioneers on processing 
full-tensor gravity-gradient data jointly.
In addition to computational feasibility,  \citeauthor{barnes-lumley2011}'s (\citeyear{barnes-lumley2011}) method reduces low-frequency noise and can also remove the drift in time-domain from the survey data. 
Those authors stressed that the $G_{zz}-$component calculated through the single estimated equivalent-layer model projected on a grid at a constant elevation by inverting full gravity-gradient data
has the low-frequency error reduced by a factor of 2.4 as compared to the inversion 
of an individual component of the gravity-gradient data.



\subsubsection{The reparametrization of the equivalent layer}

\cite{oliveirajr-etal2013} reparametrized the whole equivalent-layer model by 
a piecewise  bivariate-polynomial function defined on a set of $Q$ equivalent-source windows. 
In \citeauthor{oliveirajr-etal2013}'s (\citeyear{oliveirajr-etal2013}) approach, 
named polynomial equivalent layer (PEL), the parameter vector within the $k$th equivalent-source window 
$\mathbf{p}^{k}$  can be written in matrix notation as  
\begin{equation}
	\mathbf{p}^{k} = \mathbf{B}^{k} \: \mathbf{c}^{k} \:,  \:\:\:\:\:\:\: k \: = \: 1 \:... \: Q \:, 
	\label{eq:p_pel}
\end{equation}
where $\mathbf{p}^{k}$ is an $M_w$-dimensional vector containing the physical-property distribution
within the $k$th equivalent-source window, 
$\mathbf{c}^{k}$ is a $P$-dimensional vector whose $l$th element is the $l$th
coefficient of the $\alpha$th-order polynomial function and
$\mathbf{B}^{k}$ is an $M_w \times P$ matrix containing the first-order derivative of the 
$\alpha$th-order polynomial function with respect to one of the $P$ coefficients.

By using a regularized potential-field inversion, \cite{oliveirajr-etal2013} estimates the polynomial coefficients for each equivalent-source window by solving the following linear system
\begin{equation}
	\left( \mathbf{B}^{\top} \: \mathbf{A}^{\top} \: \mathbf{A} \: \mathbf{B}   \: +
	 \: \mu \mathbf{I} \right) \:  \mathbf{c}^{\ast}
	 \: = \: \mathbf{B}^{\top} \:  \mathbf{A}^{\top} \: \mathbf{d}^{o} \: ,
	\label{eq:linear_system_pel}
\end{equation}
where  $\mu$ is a regularizing parameter, 
$\mathbf{c}^{\ast}$ is an estimated $H$-dimensional vector containing all coefficients describing all polynomial functions within  all equivalent-source windows which compose the entire equivalent layer,
$\mathbf{I}$ is  an identity matrix of order $H (H = P \dot Q) $ and
$\mathbf{B}$ is an $M \times H$  block diagonal matrix such that the main-diagonal blocks 
are $\mathbf{B}^{k}$ matrices (equation \ref{eq:p_pel}) and all off-diagonal blocks are zero matrices.
For ease of the explanation of equation \ref{eq:linear_system_pel}, we keep only the zeroth-order Tikhonov regularization and omitting the first-order Tikhonov regularization \citep{aster2018parameter} 
which was also used  by \cite{oliveirajr-etal2013}.

The main advantage of the PEL is solve $H$-dimensional system of equations 
(equation \ref{eq:linear_system_pel}), where $H$ totalizes the number of polynomial coefficients
composing all equivalent-source windows,  requiring a lower computational effort since $H <<< N$.
To avoid the storage of matrices $\mathbf{A}$ and $\mathbf{B}$,  \cite{oliveirajr-etal2013}
evaluate an element of the matrix $\mathbf{A} \mathbf{B} $ by calculating the 
dot product between the row of matrix $\mathbf{A}$ and the column of the matrix $\mathbf{B}$. 
After estimating all polynomial coefficients of all windows, the estimated coefficients 
($\mathbf{c}^{\ast}$ in equation \ref{eq:linear_system_pel})
are transformed into a single physical-property distribution encompassing the entire equivalent layer. 

As stated by \cite{oliveirajr-etal2013}, the computational efficiency of PEL approach
stems from the fact that the total number of polynomial coefficients $H$ required to depict the 
physical-property distribution within the equivalent layer is generally much smaller than the number of equivalent sources. 
Consequently, this leads to a considerably smaller linear system that needs to be solved.
Hence, the main strategy of polynomial equivalent layer  is the model dimension reduction.

The polynomial equivalent layer was applied to perform upward continuations of gravity and magnetic data 
and reduction to the pole of magnetic data.


\subsubsection{The iterative scheme without solving a linear system}

There exists a class of methods that iteratively estimate the distribution of physical properties 
within an equivalent layer without the need to solve linear systems. 
The method initially introduced by \cite{cordell1992} and later expanded upon by \cite{guspi-novara2009} updates the physical property of sources, located beneath each potential-field data, by removing the maximum residual between the observed and fitted data. 
In addition, \cite{xia-sprowl1991} and \cite{xia-etal1993} have developed efficient iterative algorithms 
for updating the distribution of physical properties within the equivalent layer in the wavenumber and space domains, respectively.
Specifically, in \citeauthor{xia-sprowl1991}'s (\citeyear{xia-sprowl1991}) method the physical-property distribution is updated by using the ratio between the squared depth to the equivalent source and the gravitational constant multiplied by the residual between the observed and predicted observation at the measurement station. 
Neither of these methods solve linear systems.

Following this class of methods of iterative equivalent-layer technique that does not solve linear systems, \cite{siqueira-etal2017} developed a fast iterative equivalent-layer technique for processing gravity data
in which the sensitivity matrix $\mathbf{A}$ (equation \ref{eq:predicted-data-vector}) 
is replaced by a diagonal matrix $ N \times N$, i.e.:
\begin{equation}
	\mathbf{\tilde{\tilde{A}}} = 2\: \pi \: \gamma \: \mathbf{{\Delta S}^{-1}}  \: ,
	\label{eq:A_siqueira}
\end{equation}
where $\gamma$ is Newton's gravitational constant  and 
$\mathbf{{\Delta S}^{-1}}$ is a diagonal matrix of order $N$ whose diagonal elements ${\Delta s}_{i}$, $i = 1, ..., N$  are the element of area centered at the $i$th horizontal coordinates of the $i$th observation point.
The physical foundations of \citeauthor{siqueira-etal2017}'s (\citeyear{siqueira-etal2017}) method  
rely on two constraints: i) the excess of mass; and ii)  the positive correlation between the gravity observations and the mass distribution over the equivalent layer.

Although \citeauthor{siqueira-etal2017}'s (\citeyear{siqueira-etal2017}) method  does not solve any 
linear system of equations, it can be theoretically explained by solving the following linear system
at the $k$th iteration:
\begin{equation}
	\mathbf{\tilde{\tilde{A}}}^{\top} \: \mathbf{\tilde{\tilde{A}}} {{\mathbf{\Delta \: \hat{p}}} }^{k}
	 \: = \:  \mathbf{\tilde{\tilde{A}}}^{\top} \: \mathbf{r}^{k} \: ,
	\label{eq:linear_system_siqueira}
\end{equation}
where $ \mathbf{r}^{k} $ is an $N$-dimensional residual vector whose $i$th element is calculated by subtracting the  $i$th observed data $d^{o}_{i}$ from the  $i$th fitted data $d^{k}_{i}$ at the $k$th iteration, i.e.,
\begin{equation}
	r^{k}_{i} =  d^{o}_{i} \: - \: d^{k}_{i} \:.
	\label{eq:r_siqueira}
\end{equation}
and ${{\mathbf{\Delta \: \hat{p}}} }^{k}$ is an estimated $N$-dimensional vector of parameter correction.

Because $\mathbf{\tilde{\tilde{A}}}$,  in equation \ref{eq:linear_system_siqueira}, is a diagonal matrix 
(equation \ref{eq:A_siqueira}), the parameter correction estimate is directly calculated  without solving system of linear equations, and thus, an $i$th element of ${{\mathbf{\Delta \: \hat{p}}} }^{k}$  
is directly calculated by
\begin{equation}
	{{\Delta \hat{p}}^{k}}_{i} = \frac{{\Delta s}_{i} \: r^{k}_{i} } 
	{2\: \pi \: \gamma}   \: .
	\label{eq:deltap_siqueira}
\end{equation}
The mass distribution over the equivalent layer is updated by: 
\begin{equation}
	{\hat{p}}^{k+1}_{i} = {\hat{p}}^{k}_{i} \: + \: {{\Delta \hat{p}}^{k}}_{i} \: .
	\label{eq:p_siqueira}
\end{equation}
\citeauthor{siqueira-etal2017}'s (\citeyear{siqueira-etal2017}) method starts from a mass distribution 
on the equivalent layer, whose $i$th mass $p^{o}_{i}$ is proportional to the $i$th 
observed data $d^{o}_{i}$, i.e.,
\begin{equation}
	p^{o}_{i} = \frac{{\Delta s}_{i} \: d^{o}_{i} }{2\: \pi \: \gamma}   \: .
	\label{eq:po_siqueira}
\end{equation}

\cite{siqueira-etal2017} applied their fast iterative equivalent-layer 
technique to interpolate, calculate the horizontal components, and
continue upward (or downward) gravity data.

For jointly process two gravity gradient components, 
\cite{jirigalatu-ebbing2019} used the Gauss-FFT for forward calculation of potential fields in the wavenumber domain combined with Landweber's iteration coupled with a mask matrix $\mathbf{M}$ to reduce the edge effects without increasing the computation cost.
The  mask matrix $\mathbf{M}$ is defined in the following way:
if the corresponding pixel does not contain the original data, 
the element of  $\mathbf{M}$ is set to zero; otherwise, it is set to one.
The $k$th Landweber iteration is given by
\begin{equation}
	\mathbf{p}_{k+1} =	\mathbf{p}_{k} + \omega
	\left[ \mathbf{A_1}^{\top} (\mathbf{d_1} - 
	\mathbf{M} \mathbf{A_1}	\mathbf{p}_{k}) +
	\mathbf{A_2}^{\top} (\mathbf{d_2} - 
	\mathbf{M} \mathbf{A_2}	\mathbf{p}_{k}) \right]	 \:,
	\label{eq:p_jirigalatu-ebbing2019}
\end{equation}
where $\omega$ is a relaxation factor, $\mathbf{d_1}$ and $\mathbf{d_2}$
are the two gravity gradient components and $\mathbf{A_1}$ and  
$ \mathbf{A_2}$ are the corresponding gravity gradient kernels.
\cite{jirigalatu-ebbing2019} applied their method for processing
two horizontal curvature components of Falcon airborne gravity gradient.

  
\subsubsection{The convolutional equivalent layer with BTTB matrices}

\citeauthor{takahashi2020} (\citeyear{takahashi2020}, 
\citeyear{takahashi2022}) introduced the convolutional equivalent layer for gravimetric and magnetic data processing, respectively.


\cite{takahashi2020} demonstrated that the sensitivity matrix $\mathbf{A}$ (equation \ref{eq:predicted-data-vector}) associated with a planar equivalent layer formed by a set of point masses, each one directly beneath each observation point and considering a regular grid of observation points at a constant height has a symmetric block-Toeplitz Toeplitz-block (BTTB) structure.
A symmetric BTTB matrix has, at least, two attractive properties.
The first one is that it can be defined by using only
the elements forming its first column (or row).
The second attractive property is that any BTTB matrix can be
embedded into a symmetric Block-Circulat Circulant-Block (BCCB) matrix.
This means that the  full sensitivity matrix 
$\mathbf{A}$ (equation \ref{eq:predicted-data-vector})  can be 
completely reconstruct by using the first column of the BCCB matrix only.
In what follows, \cite{takahashi2020} computed the forward modeling 
by using only a single equivalent source. 
Specifically, it is done by calculating  the eigenvalues of the BCCB matrix
that can be efficiently computed by using only the first column of the BCCB matrix via 2D fast Fourier transform (2D FFT).
By comparing with the classic approach in the Fourier domain, 
the convolutional equivalent layer for gravimetric data processing 
proposed by \cite{takahashi2020} performed upward- and downward-continue gravity data with a very small border effects and noise amplification.

By using the original idea of the convolutional equivalent layer 
proposed by \cite{takahashi2020} for gravimetric data processing, \cite{takahashi2022}  developed the convolutional equivalent layer for magnetic data processing.
By assuming a regularly spaced grid of magnetic data  at a constant height 
and a planar equivalent layer of dipoles, \cite{takahashi2022} proved that the sensitivity matrix linked with this layer possess a BTTB structure in the specific scenario where each dipole is exactly beneath each observed magnetic data point. 
\cite{takahashi2022} used a conjugate gradient least-squares (CGLS) algorithm  which 
does not require an inverse matrix or matrix-matrix multiplication. 
Rather, it only requires matrix-vector multiplications per iteration, 
which can be effectively computed using the 2D FFT as a discrete convolution.
The matrix-vector product only uses the elements that constitute the first column of the associated BTTB matrix, resulting in computational time and memory savings.
\citeauthor{takahashi2022} (\citeyear{takahashi2022}) showed the robustness of
the convolutional equivalent layer  in processing magnetic survey that  
violates the requirement of regular grids in the horizontal directions
and flat observation surfaces.

The matrix-vector product in \citeauthor{takahashi2020} (\citeyear{takahashi2020}, \citeyear{takahashi2022}) 
(e.g.,  $\mathbf{d} = \mathbf{A} \mathbf{p}$, such as in equation \ref{eq:predicted-data-vector})
is the main issue to be solved.
To solve it efficiently, these authors involked the auxiliary linear system
\begin{equation}
\mathbf{w} = \mathbf{C} \mathbf{v} \: ,
\label{eq:aux_system_takahashi}
\end{equation}
where $\mathbf{w}$ and $\mathbf{v}$ are, respectively, vectors of data and parameters completed 
by zeros and $\mathbf{C}$ is a BCCB matrix formed by $2Q \times 2Q$ blocks, where each block 
$\mathbf{C}_{q}$, $q = 0, \dots, Q-1$, is a $2P \times 2P$ circulant matrix. 
The first column of $\mathbf{C}$ is obtained by rearranging the first column of
the sensitivity matrix $\mathbf{A}$ (equation \ref{eq:predicted-data-vector}).
Because a BCCB matrix is diagonalized by the 2D unitary discrete Fourier transform (DFT), 
$\mathbf{C}$ can be written as
\begin{equation}
\mathbf{C} = 
\left(\mathbf{F}_{2Q} \otimes \mathbf{F}_{2P} \right)^{\ast} 
\boldsymbol{\Lambda}
\left(\mathbf{F}_{2Q} \otimes \mathbf{F}_{2P} \right) \: ,
\label{eq:C-diagonalized_takahashi}
\end{equation}
where the symbol ``$\otimes$" denotes the Kronecker product \citep{neudecker1969},
$\mathbf{F}_{2Q}$ and $\mathbf{F}_{2P}$ are the $2Q \times 2Q$ and $2P \times 2P$ 
unitary DFT matrices \citep[][ p. 31]{davis1979}, respectively, the superscritpt 
``$\ast$" denotes the complex conjugate and $\boldsymbol{\Lambda}$ is a 
$4QP \times 4QP$ diagonal matrix containing the eigenvalues of $\mathbf{C}$.
Due to the diagonalization of the matrix $\mathbf{C}$, the auxiliary system 
(equation \ref{eq:aux_system_takahashi}) can be rewritten by using equation 
\ref{eq:C-diagonalized_takahashi} and premultiplying both sides of the result 
by $\left(\mathbf{F}_{2Q} \otimes \mathbf{F}_{2P} \right)$, i.e.,
\begin{equation}
\boldsymbol{\Lambda} \left(\mathbf{F}_{2Q} \otimes \mathbf{F}_{2P} \right) 
\mathbf{v} = \left(\mathbf{F}_{2Q} \otimes \mathbf{F}_{2P} \right) 
\mathbf{w} \: .
\label{eq:vec-DFT-system_takahashi}
\end{equation}
By applying the vec-operator \citep{takahashi2020} to both sides of 
equation \ref{eq:vec-DFT-system_takahashi}, by premultiplying  both sides of 
the result by $\mathbf{F}_{2Q}^{\ast}$ and then postmultiplying both sides of the result by 
$\mathbf{F}_{2P}^{\ast}$
\begin{equation}
\mathbf{F}_{2Q}^{\ast} \left[ 
\mathbf{L} \circ \left(\mathbf{F}_{2Q} \, \mathbf{V} \, \mathbf{F}_{2P} \right) 
\right] \mathbf{F}_{2P}^{\ast} = \mathbf{W} \: ,
\label{eq:DFT-system_takahashi}
\end{equation}
where ``$\circ$'' denotes the Hadamard product \citep[][ p. 298]{horn_johnson1991} and 
$\mathbf{L}$, $\mathbf{V}$ and $\mathbf{W}$ are $2Q \times 2P$ matrices obtained 
by rearranging, along their rows, the elements forming the diagonal of matrix 
$\boldsymbol{\Lambda}$, vector $\mathbf{v}$ and vector $\mathbf{w}$, respectively.
The left side of equation \ref{eq:DFT-system_takahashi} contains the 2D 
Inverse Discrete Fourier Transform (IDFT) of the term in brackets, which in turn
represents the Hadamard product of matrix $\mathbf{L}$ and the 2D DFT of matrix 
$\mathbf{V}$.
Matrix $\mathbf{L}$ contains the eigenvalues of $\boldsymbol{\Lambda}$ 
(equation \ref{eq:C-diagonalized_takahashi}) and can be 
efficiently computed by using only the first column of the BCCB matrix 
$\mathbf{C}$ (equation \ref{eq:aux_system_takahashi}).

Actually, in \citeauthor{takahashi2020} (\citeyear{takahashi2020}, \citeyear{takahashi2022}) 
a fast 2D discrete circular convolution \citep{vanloan1992} is used to  process 
very large gravity and magnetic datasets efficiently.
The convolutional equivalent layer was applied to perform upward continuation of large magnetic datasets. 
Compared to the classical Fourier approach, \citeauthor{takahashi2022}'s (\citeyear{takahashi2022}) method produces smaller border effects without using any padding scheme. 

Without taking advantage of the symmetric BTTB structure of the sensitivity matrix (\citeauthor{takahashi2020}, \citeyear{takahashi2020}) 
that arises when gravimetric observations are measured on a 
horizontally regular grid, on a flat surface and considering a regular grid 
of equivalent sources whithin a horizontal layer, \cite{mendoncca2020}  explored the symmetry of the gravity kernel to reduce the number of forward model evaluations.
By exploting the symmetries of the gravity kernels and redundancies in the forward model evaluations on a regular grid and combining the subspace solution based on eigenvectors of the gridded dataset, \cite{mendoncca2020}   
estimated the mass excess or deficiency produced by anomalous sources with positive or negative density contrast. 

\subsubsection{The deconvolutional equivalent layer with BTTB matrices}

To avoid the  iterations of the conjugate gradient method in \cite{takahashi2022}, we can employ 
the deconvolution process.
Equation \ref{eq:DFT-system_takahashi} shows that estimate the  matrix $\mathbf{V}$,
containing the elements of parameter vector $\mathbf{p}$, is a inverse problem that could be 
solved by deconvolution.
From equation \ref{eq:DFT-system_takahashi}, the  matrix $\mathbf{V}$ can be obtain by 
deconvolution, i.e.
\begin{equation}
\mathbf{V} \: = \:  \mathbf{F}_{2Q}^{\ast} \left[ 
\frac{\left(\mathbf{F}_{2Q} \mathbf{W} \, \mathbf{F}_{2P} \right) }{\mathbf{L}}
\right] \mathbf{F}_{2P}^{\ast} \: .
\label{eq:deconvolution_takahashi}
\end{equation}
Equation \ref{eq:deconvolution_takahashi} shows that the parameter vector (in matrix $\mathbf{V}$) 
can be theoretically obtain by dividing each potential-field observations (in matrix $\mathbf{W}$)
by each eigenvalues (in matrix $\mathbf{L}$).
Hence, the parameter vector is constructed by element-by-element division of data by eigenvalues.

However, the deconvolution often is extremely unstable. 
This means that a small change in data can lead to an enormous change in the estimated parameter.
Hence, equation \ref{eq:deconvolution_takahashi} requires regularization to be useful.
We usede wiener deconvolution to obtain a stable solution, i.e.,
\begin{equation}
\mathbf{V} \: = \:  \mathbf{F}_{2Q}^{\ast} \left[ 
\left(\mathbf{F}_{2Q} \mathbf{W} \, \mathbf{F}_{2P} \right)  
\frac{\mathbf{L}^{\ast}}{ \left(\mathbf{L} \: \mathbf{L}^{\ast} + \mu \right)}
\right] \mathbf{F}_{2P}^{\ast} \: ,
\label{eq:wiener_takahashi}
\end{equation}
where the matrix $\mathbf{L}^{\ast}$ contains the complex conjugate eigenvalues and
$\mu$ is a parameter that controls the degree of stabilization. 


\subsection{Solution stability}

The solution stability of the equivalent-layer methods is rarely addressed.
Here, we follow the numerical stability analysis presented in \cite{siqueira-etal2017}.

Let us assume noise-free potential-field data $\mathbf{d}$, 
we estimate a physical-property distribution $\mathbf{p}$ (estimated solution) within the equivalent layer.
Then, the  noise-free data $\mathbf{d}$ are contaminated with additive $D$ different sequences of 
pseudorandom Gaussian noise, creating  different noise-corrupted potential-field data
$\mathbf{d}^\mathbf{o}_\ell$, $\ell = 1, ..., D$.
From each $\mathbf{d}^\mathbf{o}_\ell$, we estimate a physical-property distribution 
$\mathbf{\hat{p}}_\ell$ within the equivalent layer. 

Next, for each noise-corrupted data $\mathbf{d}^\mathbf{o}_\ell$
and estimated solution $\mathbf{\hat{p}}_\ell$, the $\ell$th model perturbation $\delta p_\ell $
and the $\ell$th data perturbation    $\delta d_\ell$ are,  respectively, evaluated by
\begin{equation}
\delta p_\ell = \frac{\parallel \mathbf{\hat{p}}_\ell - {\mathbf{p}} \parallel_2 }
{\parallel {\mathbf{p}} \parallel _2 }, \quad \ell= 1, ..., D,
\label{del_p}
\end{equation}
and
\begin{equation}
\delta d_\ell = \frac{\parallel \mathbf{d}^\mathbf{o}_\ell - \mathbf{d} \parallel _2 }
{\parallel \mathbf{d}\parallel _2}, \quad \ell = 1, ..., D.
\label{del_d}
\end{equation}

Regardless of the particular method used, the following inequality \citep[][ p. 66]{aster2018parameter}  is applicable:
\begin{equation}
\delta p_\ell \leq \kappa \; \delta d_\ell, \quad \ell = 1, ..., D,
\label{condition_number}
\end{equation}
where $\kappa$ is the constant of proportionality between the model perturbation $\delta p_\ell $ 
(equation \ref{del_p}) and the data perturbation   $\delta d_\ell$ (equation \ref{del_d}).
The constant $\kappa$ acts as the condition number of an invertible matrix in a given inversion, and thus
measures the instability of the solution.
The larger (smaller) the value of $\kappa$ the more unstable (stable) is the estimated solution.

Equation \ref{condition_number} shows a linear relationship between the model perturbation and 
the data perturbation.
By plotting $\delta p_\ell$ (equation \ref{del_p}) against $\delta d_\ell$ (equation \ref{del_d}) 
produced by a set of $D$ estimated solution obtained by applying a given equivalent-layer method, 
we obtain a straight line behaviour described by equation \ref{condition_number}.
By applying a linear regression, we obtain a fitted straight line whose estimated slope 
($\kappa$ in equation \ref{condition_number}) quantifies the solution stability.

Here, the  analysis of solution stability is numerically conducted by applying
the classical equivalent-layer technique with zeroth-order Tikhonov regularization,
the convolutional method for gravimetric and magnetic data,
the deconvolutional method (equation \ref{eq:deconvolution_takahashi}) and 
the deconvolutional method with different values for the Wiener stabilization 
(equation \ref{eq:wiener_takahashi}).
