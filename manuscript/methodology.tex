%% Methodology
\section{Fundamentals}

%Consider a set of $N$ potential-field observations (gravity or magnetic data) 
%$d^{o}_{i}$ $(x_{i}, y_{i}, z_{i})$, $i =  1, \dots, N $, 
%at the $i$th observation point $(x_{i}, y_{i}, z_{i})$ of a Cartesian 
%coordinate system with $x$-, $y$- and $z$-axis pointing to north, east and down, respectively.
%Physically, the discrete set of potential-field observations is produced by a unknown source distribution in the subsurface.
%Mathematically, it represents a discrete set of a harmonic function.
%
%A standard way to deal with the classical equivalent-layer technique is approximate the observed potential-field data by the predicted data,
%which in turn are produced by a fictitious layer of sources, 
%called equivalent layer.
%The equivalent layer is located below the observation surface, at depth 
%$z_0$ ($z_0 > z_i$), and with finite horizontal dimensions being composed by 
%a finite discrete set of equivalent sources (e.g., point masses, dipoles, or prisms). 
%Mathematically, this approximation can be written in matrix notation as
%\begin{equation}
%	\mathbf{d} = \mathbf{A} \mathbf{p} \: ,
%	\label{eq:predicted-data-vector}
%\end{equation}
%where $\mathbf{d}$ is an $N$-dimensional predicted data vector whose $i$th element, $d_{i}$ $(x_{i}, y_{i}, z_{i})$, $i =  1, \dots, N $, is the 
%predicted potential-field observation,
%$\mathbf{p}$ is an $M$-dimensional parameter vector whose $j$th element $p_{j}$
%can be a physical property of the $j$th equivalent source and $\mathbf{A}$ is the $N \times M$ sensitivity matrix whose  $ij$th element $a_{ij}$ is a  harmonic function.
%
%
%\subsection{Computational strategies}
%
%The classical equivalent-layer technique consists of estimating the parameter vector $\mathbf{p}$ from the $N$-dimensional observed data vector 
%$\mathbf{d^{o}}$ whose $i$th element is defined as the 
%$d^{o}_{i}$ $(x_{i}, y_{i}, z_{i})$, $i =  1, \dots, N $.
%Usually, this estimate can be obtained by a regularized least-squares solution 
%The estimated parameter is stable, fits the observed data and can be used to
%yield a desired linear transformation of the data, such as interpolation, upward (or downward) continuation, reduction to the pole,  joint processing of
%gravity gradient data and more.
%Mathematically, the desired linear transformation of the data can be obtained by
%\begin{equation}
%	\hat{\mathbf{t}} = \mathbf{T} \mathbf{p^{\ast}}\: ,
%	\label{eq:t_data}
%\end{equation}
%where $\hat{\mathbf{t}}$ is an $N$-dimensional transformed data vector,
%$\mathbf{p^{\ast}}$ is an $M$-dimensional estimated parameter vector and
%$\mathbf{T}$ is the $N \times M$ matrix of Green's functions whose  $ij$th element  is the transformed field at the $i$th observation point produced by the $j$th equivalent source.
%
%The biggest hurdle to use the classical equivalent-layer technique 
%is the computational complexity to handle large datasets 
%because the sensitivity matrix  $\mathbf{A}$ 
%(equation \ref{eq:predicted-data-vector}) is dense. 
%Usually, the estimated parameter vector $\mathbf{p^{\ast}}$
%requires to solve a large-scale linear inversion which in turn means to deal with some obstacles concerning large computational cost:
%i)   the large computer memory to store large and full matrices;
%ii)  the long computation time to mutiply a matrix by a vector; and
%iii) the long computation time to solve a large linear system of equations.
%
%
%Here, we review some strategies for reducing the computational cost of equivalent-layer technique.
%These strategies are the following:

Let $\mathbf{d}$ be a $D \times 1$ vector, whose $i$-th element $d_{i}$ is the observed potential
field at the position $(x_{i}, y_{i}, z_{i})$, $i \in \{1:D\}$.
Consider that $d_{i}$ can be satisfactorily approximated by a harmonic function
\begin{equation}
	f_{i} = \sum\limits_{j = 1}^{P} g_{ij} \, p_{j} \: ,
	\quad i \in \{1:D\} \: ,
	\label{eq:predicted-data-f-i}
\end{equation}
where, $p_{j}$ represents the scalar physical property of a virtual source (i.e., monopole, dipole, prism) located
at $(x_{j}, y_{j}, z_{j})$, $j \in \{1:P\}$ and 
\begin{equation}
	g_{ij} \equiv g(x_{i} - x_{j}, y_{i} - y_{j}, z_{i} - z_{j}) \: ,
	\quad z_{i} < \min\{z_{j}\} \: , \quad \forall i \in \{1:D\} \: ,
	\label{eq:harmonic-function-g-ij}
\end{equation}
is a harmonic function, where $\min\{z_{j}\}$ denotes the minimum $z_{j}$, or the vertical coordinate of the 
shallowest virtual source.
These virtual sources are called \textit{equivalent sources} and they form an \textit{equivalent layer}.
In matrix notation, the potential field produced by all equivalent sources at all points 
$(x_{i}, y_{i}, z_{i})$, $i \in \{1:D\}$, is given by:
\begin{equation}
	\mathbf{f} = \mathbf{G} \mathbf{p} \: ,
	\label{eq:predicted-data-vector}
\end{equation}
where $\mathbf{p}$ is a $P \times 1$ vector with $j$-th element $p_{j}$ representing the scalar physical property
of the $j$-th equivalent source 
and $\mathbf{G}$ is a $D \times P$ matrix with element $g_{ij}$ given by equation \ref{eq:harmonic-function-g-ij}. 

The equivalent-layer technique consists in solving a linear inverse problem to determine a parameter vector $\mathbf{p}$ 
leading to a predicted data vector $\mathbf{f}$ (equation \ref{eq:predicted-data-vector}) \textit{sufficiently close to} the 
observed data vector $\mathbf{d}$, whose $i$-th element $d_{i}$ is the observed potential field at $(x_{i}, y_{i}, z_{i})$.
The notion of \textit{closeness} is intrinsically related to the concept of \textit{vector norm} \cite[e.g.,][p. 68]{golub-vanloan2013}
or \textit{measure of length} \cite[e.g.,][p. 41]{menke2018}.
Because of that, almost all methods for determining $\mathbf{p}$ actually estimate a parameter 
vector $\tilde{\mathbf{p}}$ minimizing a length measure of the difference between $\mathbf{f}$ and $\mathbf{d}$
(see subsection \ref{subsec:general-formulation}).
Given an estimate $\tilde{\mathbf{p}}$, it is then possible to compute a potential field transformation 
\begin{equation}
	\mathbf{t} = \mathbf{A} \tilde{\mathbf{p}} \: ,
	\label{eq:transformation}
\end{equation}
where $\mathbf{t}$ is a $T \times 1$ vector with $k$-th element $t_{k}$ representing the transformed potential field at
the position $(x_{k}, y_{y}, z_{k})$, $k \in \{1:T\}$, and
\begin{equation}
	a_{kj} \equiv a(x_{k} - x_{j}, y_{k} - y_{j}, z_{k} - z_{j}) \: ,
	\quad z_{k} < \min\{z_{j}\} \: , \quad \forall k \in \{1:T\} \: ,
	\label{eq:harmonic-function-a-kj}
\end{equation}
is a harmonic function representing the $kj$-th element of the $T \times P$ matrix $\mathbf{A}$.

\subsection{Spatial distribution and total number of equivalent sources}
\label{subsec:spatial-distribution-sources}

There is no well-established criteria to define the optimum number $P$ or the spatial distribution
of the equivalent sources. We know that setting an equivalent layer with more (less) sources than potential-field 
data usually leads to an underdetermined (overdetermined) inverse problem \cite[e.g.,][ p. 52--53]{menke2018}.
Concerning the spatial distribution of the equivalent sources, the only condition is that they must rely on a 
surface that is located below and does not cross that containing the potential field data.
\citet{soler-uieda2021} present a practical discussion about this topic.

From a theoretical point of view, the equivalent layer reproducing a given potential field data set cannot cross the
true gravity or magnetic sources. This condition is a consequence of recognizing that the equivalent layer is essentially an indirect solution of 
a boundary value problem of potential theory \citep[e.g.,][]{roy1962,zidarov1965,dampney1969,li_etal_2014,reis-etal2020}.
In practical applications, however, there is no guarantee that this condition is satisfied. 
Actually, its is widely known from practical experience \cite[e.g.,][]{gonzalez-etal2022} that the equivalent-layer technique
works even for the case in which the layer cross the true sources. 

CRITÉRIOS PARA DEFINIR A PROFUNDIDADE DA CAMADA: DAMPNEY (ESPAÇAMENTO DO GRID) E REIS (ESPAÇAMENTO DAS LINHAS)

\subsection{Matrix $\mathbf{G}$}
\label{subsec:sensitivity-matrix}

Generally, the harmonic function $g_{ij}$ (equation \ref{eq:harmonic-function-g-ij}) is defined in terms of the 
inverse distance between the observation point $(x_{i}, y_{i}, z_{i})$ and the $j$-th equivalent source at $(x_{j}, y_{j}, z_{j})$,
\begin{equation}
	\frac{1}{r_{ij}} \equiv \frac{1}{\sqrt{(x_{i} - x_{j})^{2} + (y_{i} - y_{j})^{2} + (z_{i} - z_{j})^{2}}} \: ,
	\label{eq:inverse-distance-ij}
\end{equation}
or by its partial derivatives of first and second orders, respectively given by
\begin{equation}
	\partial_{\alpha} \frac{1}{r_{ij}} \equiv \frac{-(\alpha_{i} - \alpha_{j})}{r_{ij}^{3}} \: ,
	\quad \alpha \in \{ x, y, z \} \: ,
	\label{eq:deriv-1-inverse-distance-ij}
\end{equation}
and
\begin{equation}
	\partial_{\alpha\beta} \frac{1}{r_{ij}} \equiv 
	\begin{cases}
		\frac{3 \, (\alpha_{i} - \alpha_{j})^{2}}{r_{ij}^{5}} \: , &\alpha = \beta \: , \\
		\frac{3 \, (\alpha_{i} - \alpha_{j}) \, (\beta_{i} - \beta_{j})}{r_{ij}^{5}} - \frac{1}{r_{ij}^{3}} \: , &\alpha \ne \beta \: , \\
	\end{cases}
	\quad \alpha, \beta \in \{ x, y, z \} \: .
	\label{eq:deriv-2-inverse-distance-ij}
\end{equation}
In this case, the equivalent layer is formed by punctual sources representing monopoles or dipoles
\cite[e.g.,][]{dampney1969, emilia1973, leao-silva1989, cordell1992, oliveirajr-etal2013, siqueira-etal2017, reis-etal2020, takahashi-etal2020, soler-uieda2021, takahashi-etal2022}.
Another common approach consists in not defining $g_{ij}$ by using equations \ref{eq:inverse-distance-ij}--\ref{eq:deriv-2-inverse-distance-ij},
but other harmonic functions obtained by integrating them over the volume of regular prisms 
\cite[e.g.,][]{li-oldenburg_2010, barnes-lumley_2011, li_etal_2014, jirigalatu-ebbing2019}.
There are also some less common approaches defining the harmonic function $g_{ij}$ (equation \ref{eq:harmonic-function-g-ij})
as the potential field due to plane faces with constant physical property \citep{hansen-miyazaki1984}, doublets \citep{silva1986} or
by computing the double integration of the inverse distance function with respect to $z$ \citep{guspi-novara2009}.

A common assumption for most of the equivalent-layer methods is that the harmonic function $g_{ij}$ 
(equation \ref{eq:harmonic-function-g-ij}) is independent on the actual physical relationship between the
observed potential field and their true sources \cite[e.g.,][]{cordell1992, guspi-novara2009,li_etal_2014}.
Hence, $g_{ij}$ can be defined according to the problem.
The only condition imposed to this function is that it decays to zero as the observation point $(x_{i}, y_{i}, z_{i})$
goes away from the position $(x_{j}, y_{j}, z_{j})$ of the $j$-th equivalent source.
However, several methods use a function $g_{ij}$ that preserves the physical relationship between the
observed potential field and their true sources.
For the case in which the observed potential field is gravity data, $g_{ij}$ is commonly defined as a component of 
the gravitational field produced at $(x_{i}, y_{i}, z_{i})$ by a point mass or prism located at $(x_{j}, y_{j}, z_{j})$, with unit density.
On the other hand, $g_{ij}$ is commonly defined as a component of the 
magnetic induction field produced at $(x_{i}, y_{i}, z_{i})$ by a dipole or prism located at $(x_{j}, y_{j}, z_{j})$,
with unit magnetization intensity, when the observed potential field is magnetic data.

For all harmonic functions discussed above, the sensitivity matrix $\mathbf{G}$ (equation \ref{eq:predicted-data-vector}) 
is always dense. For scattered potential-field data, $\mathbf{G}$ does not have a well-defined structure, regardless of
whether the spatial distribution of the equivalent sources is set.
Nevertheless, for the particular case in which (i) there is a single equivalent source right below each potential-field
datum and (ii) both data and sources rely on planar and regularly spaced grids, \citet{takahashi-etal2020,takahashi-etal2022}
show that $\mathbf{G}$ assumes a block-Toeplitz Toeplitz-block (BTTB) structure. In this case, the product of $\mathbf{G}$ and an 
arbitrary vector can be efficiently computed via 2D fast Fourier transform as a discrete convolution.

\subsection{General formulation}
\label{subsec:general-formulation}

A general formulation for almost all equivalent-layer methods can be achieved by first considering 
that the $P \times 1$ parameter vector $\mathbf{p}$ (equation \ref{eq:predicted-data-vector}) can be reparameterized 
into a $Q \times 1$ vector $\mathbf{q}$ according to:
\begin{equation}
	\mathbf{p} = \mathbf{H} \, \mathbf{q} \: ,
	\label{eq:reparameterization}
\end{equation}
where $\mathbf{H}$ is a $P \times Q$ matrix.
The predicted data vector $\mathbf{f}$ (equation \ref{eq:predicted-data-vector}) can then be
rewritten as follows:
\begin{equation}
	\mathbf{f} = \mathbf{G} \, \mathbf{H} \, \mathbf{q} \: .
	\label{eq:predicted-data-vetor-reparameterized}
\end{equation}

Then, the problem of estimating a parameter vector $\tilde{\mathbf{p}}$ minimizing a length 
measure of the difference between $\mathbf{f}$ (equation \ref{eq:predicted-data-vector}) and $\mathbf{d}$
is replaced by that of estimating an auxiliary vector $\tilde{\mathbf{q}}$ minimizing the goal function
\begin{equation}
	\Gamma(\mathbf{q}) = \Phi(\mathbf{q}) + \mu \: \Theta(\mathbf{q}) \: ,
	\label{eq:function-Gamma}
\end{equation}
which is a combination of particular measures of length given by
\begin{equation}
	\Phi(\mathbf{q}) = \left( \mathbf{d} - \mathbf{f} \right)^{\top}\mathbf{W}_{d}\left( \mathbf{d} - \mathbf{f} \right) \: ,
	\label{eq:function-Phi}
\end{equation}
and
\begin{equation}
	\Theta(\mathbf{q}) = \left( \mathbf{q} - \bar{\mathbf{q}} \right)^{\top}\mathbf{W}_{q}\left( \mathbf{q} - \bar{\mathbf{q}} \right) \: ,
	\label{eq:function-Theta}
\end{equation}
where $\mu$ is a positive scalar controlling the trade-off between $\Phi(\mathbf{q})$ and $\Theta(\mathbf{q})$; 
$\mathbf{W}_{q}$ is a $Q \times Q$ symmetric matrix imposing prior information on $\mathbf{q}$ given by
\begin{equation}
	\mathbf{W}_{q} = \mathbf{H}^{\top} \mathbf{W}_{p} \, \mathbf{H} \: ,
	\label{eq:weighting-matrix-q}
\end{equation}
with $\mathbf{W}_{p}$ being a $P \times P$ symmetric matrix imposing prior information on $\mathbf{p}$;
$\bar{\mathbf{q}}$ is a $Q \times 1$ vector of reference values for $\mathbf{q}$ satisfying
\begin{equation}
	\bar{\mathbf{p}} = \mathbf{H} \, \bar{\mathbf{q}} \: ,
	\label{eq:reparameterization-reference}
\end{equation}
with $\bar{\mathbf{p}}$ being a $P \times 1$ vector containing reference values
for the original parameter vector $\mathbf{p}$; and 
$\mathbf{W}_{d}$ is a $D \times D$ symmetric matrix defining the relative importance of each observed datum $d_{i}$.
After obtaining an estimate $\tilde{\mathbf{q}}$ for the reparameterized parameter vector $\mathbf{q}$ (equation \ref{eq:reparameterization}) minimizing
$\Gamma(\mathbf{q})$ (equation \ref{eq:function-Gamma}), the estimate $\tilde{\mathbf{p}}$ for the original parameter vector 
(equation \ref{eq:predicted-data-vector}) is computed by 
\begin{equation}
	\tilde{\mathbf{p}} = \mathbf{H} \, \tilde{\mathbf{q}} \: .
	\label{eq:vector-p-tilde}
\end{equation}

The reparameterized vector $\tilde{\mathbf{q}}$ is obtained by first computing the gradient of $\Gamma(\mathbf{q})$,
\begin{equation}
	\boldsymbol{\nabla} \Gamma(\mathbf{q}) = 
	-2 \, \mathbf{H}^{\top}\mathbf{G}^{\top} \mathbf{W}_{d} \left(\mathbf{d} - \mathbf{f} \right) +
	2 \, \mu \, \mathbf{W}_{q} \left( \mathbf{q} - \bar{\mathbf{q}} \right) \: .
	\label{eq:gradient-Gamma}
\end{equation}
Then, by considering that $\boldsymbol{\nabla} \Gamma(\tilde{\mathbf{q}}) = \mathbf{0}$ (equation \ref{eq:gradient-Gamma}),
where $\mathbf{0}$ is a vector of zeros, as well as adding and subtracting the term
$\left( \mathbf{H}^{\top}\mathbf{G}^{\top}\mathbf{W}_{d} \mathbf{G} \, \mathbf{H} \right) \bar{\mathbf{q}}$ ,
we obtain
\begin{equation}
	\tilde{\boldsymbol{\delta}}_{q} = \mathbf{B} \, \tilde{\boldsymbol{\delta}}_{d} \: ,
	\label{eq:vector-q-tilde}
\end{equation}
where 
\begin{equation}
	\tilde{\boldsymbol{\delta}}_{q} = \tilde{\mathbf{q}} - \bar{\mathbf{q}} \: ,
	\label{eq:delta-q-tilde}
\end{equation}
\begin{equation}
	\tilde{\boldsymbol{\delta}}_{d} = \mathbf{d} - \mathbf{G} \, \mathbf{H} \, \bar{\mathbf{q}} \: ,
	\label{eq:delta-d}
\end{equation}
\begin{equation}
	\mathbf{B} = \left( \mathbf{H}^{\top} \mathbf{G}^{\top} \mathbf{W}_{d} \, \mathbf{G} \, \mathbf{H} + 
	\mu \, \mathbf{W}_{q} \right)^{-1}
	\mathbf{H}^{\top} \mathbf{G}^{\top} \mathbf{W}_{d} \: ,
	\label{eq:matrix-B-overdetermined}
\end{equation}
or, equivalently \cite[][p. 62]{menke2018},
\begin{equation}
	\mathbf{B} = \mathbf{W}_{q}^{-1} \, \mathbf{H}^{\top} \mathbf{G}^{\top}
	\left( \mathbf{G} \, \mathbf{H} \, \mathbf{W}_{q}^{-1} \,
	\mathbf{H}^{\top}\mathbf{G}^{\top} + \mu \mathbf{W}_{d}^{-1} \right)^{-1} \: .
	\label{eq:matrix-B-underdetermined}
\end{equation}
Evidently, we have considered that all inverses exist in equations \ref{eq:matrix-B-overdetermined} and \ref{eq:matrix-B-underdetermined}.

Matrix $\mathbf{B}$ defined by equation \ref{eq:matrix-B-overdetermined} is commonly used for the cases in which $D > P$, i.e., when
there are more data than parameters (overdetermined problems).
In this case, we consider that the estimate $\tilde{\mathbf{q}}$ is obtained by solving the following linear system
for $\tilde{\boldsymbol{\delta}}_{q}$ (equation \ref{eq:delta-q-tilde}):
\begin{equation}
	\left( \mathbf{H}^{\top} \mathbf{G}^{\top} \mathbf{W}_{d} \, \mathbf{G} \, \mathbf{H} + 
	\mu \, \mathbf{W}_{q} \right) 
	\tilde{\boldsymbol{\delta}}_{q} = 
	\mathbf{H}^{\top} \mathbf{G}^{\top} \mathbf{W}_{d} \: 
	\tilde{\boldsymbol{\delta}}_{d} \: .
	\label{eq:delta-q-tilde-overdetermined}
\end{equation}
On the other hand, for the cases in which $D < P$ (underdetermined problems), matrix $\mathbf{B}$ is 
usually defined according to equation \ref{eq:matrix-B-underdetermined}. In this case, we consider that the 
the estimate $\tilde{\mathbf{q}}$ is obtained in two steps, which consists in first solving a linear system 
for a dummy vector $\mathbf{u}$ and then computing a matrix-vector product as follows:
\begin{equation}
	\begin{split}
		\left( \mathbf{G} \, \mathbf{H} \, \mathbf{W}_{q}^{-1} \,
		\mathbf{H}^{\top}\mathbf{G}^{\top} + \mu \, \mathbf{W}_{d}^{-1} \right) \,  
		\mathbf{u} = \tilde{\boldsymbol{\delta}}_{d} \\
		\tilde{\boldsymbol{\delta}}_{q} = \mathbf{W}_{q}^{-1}
		\mathbf{H}^{\top} \mathbf{G}^{\top} \mathbf{u}
	\end{split} \: .
	\label{eq:delta-q-tilde-underdetermined}
\end{equation}
After obtaining $\tilde{\boldsymbol{\delta}}_{q}$ (equations \ref{eq:delta-q-tilde-overdetermined} and \ref{eq:delta-q-tilde-underdetermined}),
the estimate $\tilde{\mathbf{q}}$ is computed with equation \ref{eq:delta-q-tilde}.

\section{Computational strategies}

COMEÇAR EXPLICANDO AS LIMITAÇÕES DA SOLUÇÃO OBTIDA PELA FORMULAÇÃO GERAL

Two important factors affecting the efficiency of a given matrix algorithm are the 
storage and amount of required arithmetic. Here, we quantify this last factor by counting flops.
A flop is a floating point addition, subtraction, multiplication or division \cite[][ p. 12--14]{golub-vanloan2013}.

To investigate the efficiency of equivalent-layer methods, we consider how they:
\begin{itemize}
	\item[(i)] set up the linear system (equations \ref{eq:delta-q-tilde-overdetermined} and \ref{eq:delta-q-tilde-underdetermined});
	\item[(ii)] solve the linear system (equations \ref{eq:delta-q-tilde-overdetermined} and \ref{eq:delta-q-tilde-underdetermined});
	\item[(iii)] perform potential-field transformations (equation \ref{eq:transformation}).
\end{itemize}
We focus on the overall strategies used by the selected methods.

\subsection{Notation for subvectors and submatrices}

Here, we use a notation inspired on that presented by \cite[][p. 4]{vanloan1992} to represent subvectors and submatrices.
Subvectors of $\mathbf{d}$, for example, are specified by $\mathbf{d}[\mathbf{i}]$, where $\mathbf{i}$ is a
list of integer numbers that ``pick out'' the elements of $\mathbf{d}$ forming the subvector $\mathbf{d}[\mathbf{i}]$.
For example, $\mathbf{i} = (1, 6, 4, 6)$ gives the subvector $\mathbf{d}[\mathbf{i}] = [ d_{1} \:\: d_{6} \:\: d_{4} \:\: d_{6} ]^{\top} $.
Note that the list $\mathbf{i}$ of indices may be sorted or not and it may also have repeated indices.
The list may also have a single element $\mathbf{i} = (i)$, which results in the $i$-th element $d_{i} \equiv \mathbf{d}[i]$ of $\mathbf{d}$.
We may also define regular lists of indices by using the colon notation. For example, 
\begin{equation*}
	\begin{split}
		\mathbf{i} = (3:8) &\Leftrightarrow \mathbf{d}[3:8] = [ d_{3} \:\: d_{4} \:\: \dots \:\: d_{8} ]^{\top} \\
		\mathbf{i} = (:8) &\Leftrightarrow \mathbf{d}[:8] = [ d_{1} \:\: d_{2} \:\: \dots \:\: d_{7} ]^{\top} \\
		\mathbf{i} = (3:) &\Leftrightarrow \mathbf{d}[3:] = [ d_{3} \:\: d_{4} \:\: \dots \:\: d_{D} ]^{\top} \\
	\end{split} \quad ,
\end{equation*}
where $D$ is the number of elements forming $\mathbf{d}$.

The notation above can also be used to define submatrices of the $D \times P$ matrix $\mathbf{G}$. 
For example, $\mathbf{i} = (2, 7, 4, 6)$ and $\mathbf{j} = (1, 3, 8)$ lead to the submatrix
\begin{equation*}
	\mathbf{G}[\mathbf{i}, \mathbf{j}] = \begin{bmatrix}
		g_{21} & g_{23} & g_{28} \\
		g_{71} & g_{73} & g_{78} \\
		g_{41} & g_{43} & g_{48} \\
		g_{61} & g_{63} & g_{68} 
	\end{bmatrix} \: .
\end{equation*}
Note that, in this case, the lists $\mathbf{i}$ and $\mathbf{j}$ ``pick out'', respectively, the rows and columns
of $\mathbf{G}$ that form the submatrix $\mathbf{G}[\mathbf{i}, \mathbf{j}]$.
The $i$-th row of $\mathbf{G}$ is given by the $1 \times P$ vector $\mathbf{G}[i,:]$.
Similarly, the $D \times 1$ vector $\mathbf{G}[:,j]$ represents the $j$-th column.
Finally, we may use the colon notation to define the following submatrix:
\begin{equation*}
	\mathbf{G}[2:5,3:7] = \begin{bmatrix}
		g_{23} & g_{24} & g_{25} & g_{26} & g_{27} \\
		g_{33} & g_{34} & g_{35} & g_{36} & g_{37} \\
		g_{43} & g_{44} & g_{45} & g_{46} & g_{47} \\
		g_{53} & g_{54} & g_{55} & g_{56} & g_{57}
	\end{bmatrix} \: ,
\end{equation*}
which contains the contiguous elements of $\mathbf{G}$ from rows $2$ to $5$ and from columns
$3$ to $7$.

\subsection{Moving window}

The initial approach to enhance the computational efficiency of the equivalent-layer technique 
is commonly denoted \textit{moving window} and involves
first splitting the observed data $d_{i}$, $i \in \{1 : D\}$, into $M$ overlapping subsets (or data windows) 
formed by $D^{m}$ data each, $m \in \{ 1 : M \}$.
The data inside the $m$-th window are usually adjacent to each other and have indices defined by an 
integer list $\mathbf{i}^{m}$ having $D^{m}$ elements.
The number of data $D^{m}$ forming the data windows are not necessarily equal to each other.
Each data window has a $D^{m} \times 1$ observed data vector $\mathbf{d}^{m} \equiv \mathbf{d}[\mathbf{i}^{m}]$.
The second step consists in defining a set of $P$ equivalent sources with scalar physical property $p_{j}$, $j \in \{1:P\}$,
and also split them into $M$ overlapping subsets (or source windows) formed by $P^{m}$ data each, $m \in \{ 1 : M \}$.
The sources inside the $m$-th window have indices defined by an integer list $\mathbf{j}^{m}$ having $P^{m}$ elements.
Each source window has a $P^{m} \times 1$ parameter vector $\mathbf{p}^{m}$ and
is located right below the corresponding $m$-th data window. 
Then, each $\mathbf{d}^{m} \equiv \mathbf{d}[\mathbf{i}^{m}]$ is approximated by 
\begin{equation}
	\mathbf{f}^{m} = \mathbf{G}^{m} \mathbf{p}^{m} \: ,
	\label{eq:predicted-data-window-m}
\end{equation}
where $\mathbf{G}^{m} \equiv \mathbf{G}[\mathbf{i}^{m}, \mathbf{j}^{m}]$ is a submatrix of 
$\mathbf{G}$ (equation \ref{eq:predicted-data-vector}) formed by the elements computed with equation 
\ref{eq:harmonic-function-g-ij} using only the data and equivalent sources located inside the window $m$-th.
The main idea of the moving-window approach is using the $\tilde{\mathbf{p}}^{m}$ estimated for 
each window to obtain (i) an estimate $\tilde{\mathbf{p}}$ of the parameter vector for the entire equivalent layer
or (ii) a given potential-field transformation $\mathbf{t}$ (equation \ref{eq:transformation}).
The main advantages of this approach is that (i) the estimated parameter vector $\tilde{\mathbf{p}}$ or transformed potential field
are not obtained by solving the full, but smaller linear systems and (ii) the full matrix $\mathbf{G}$ (equation \ref{eq:predicted-data-vector})
is never stored.

\cite{leao-silva1989} presented a pioneer work using the moving-window approach.
Their method requires a regularly-spaced grid of observed data on a horizontal plane $z_{0}$. 
The data windows are defined by square local grids of $\sqrt{D'} \times \sqrt{D'}$ adjacent points, all of them having the
same number of points $D'$.
The equivalent sources in the $m$-th data window are located below the observation plane, at a constant vertical distance
$\Delta z_{0}$. They are arranged on a regular grid of $\sqrt{P'} \times \sqrt{P'}$ adjacent points 
following the same grid pattern of the observed data. 
The local grid of sources for all data windows have the same number of elements $P'$.
Besides, they are vertically aligned, but expands the limits of their corresponding data windows,
so that $D' < P'$.
Because of this spatial configuration of observed data and equivalent sources, we have that
$\mathbf{G}^{m} = \mathbf{G}'$ (equation \ref{eq:predicted-data-window-m}) for all data windows 
(i.e., $\forall \: m \in \{1 : M\}$), where $\mathbf{G}'$ is a $D' \times P'$ constant matrix.

By omitting the normalization strategy used by \cite{leao-silva1989}, their method consists in 
directly computing the transformed potential field $t^{m}_{c}$ at the central point $(x^{m}_{c}, y^{m}_{c}, z_{0} + \Delta z_{0})$ 
of each data window as follows:
\begin{equation}
	t^{m}_{c} = \left( \mathbf{G}' \mathbf{a}' \right)^{\top} 
	\left[ \mathbf{G}' \, \left( \mathbf{G}' \right)^{\top} + \mu \, \mathbf{I}_{D'} \right]^{-1} 
	\mathbf{d}^{m} \: , \quad m \in \{ 1 : M \} \: ,
	\label{eq:transformed-field-tmc-LS89}
\end{equation}
where $\mathbf{I}_{D'}$ is the identity matrix of order $D'$ and
$\mathbf{a}'$ is a $P' \times 1$ vector with elements computed by equation 
\ref{eq:harmonic-function-a-kj} by using all equivalent sources in the $m$-th subset and
only the coordinate of the central point in the $m$-th data window.
Due to the presumed spatial configuration of the observed 
data and equivalent sources, $\mathbf{a}'$ is the same for all data windows.
Note that equation \ref{eq:transformed-field-tmc-LS89} combines the potential-field transformation
(equation \ref{eq:transformation}) with the solution of the undetermined problem
(equation \ref{eq:delta-q-tilde-underdetermined}) for the particular case in which 
$\mathbf{H} = \mathbf{W}_{p} = \mathbf{I}_{P'}$ (equations \ref{eq:reparameterization} and \ref{eq:weighting-matrix-q}),
$\mathbf{W}_{d} = \mathbf{I}_{D'}$ (equation \ref{eq:function-Phi}), $\bar{p} = \mathbf{0}$ 
(equation \ref{eq:reparameterization-reference}), 
where $\mathbf{I}_{P'}$ and $\mathbf{I}_{D'}$ are identity matrices 
of order $P'$ and $D'$, respectively, and $\mathbf{0}$ is a vector of zeros. 

The method proposed by \cite{leao-silva1989} can be outlined by the Algorithm \ref{alg:LS89}.
Note that \cite{leao-silva1989} directly compute the transformed potential $t^{m}_{c}$ at the central point of
each data window without explicitly computing and storing an estimated for $\mathbf{p}^{m}$ (equation \ref{eq:predicted-data-window-m}).
It means that their method allows computing a single potential-field transformation. 
A different transformation or the same one evaluated at different points require running their moving-data window method again.

\begin{algorithm}
	%\SetAlgoLined
	\Input{}
	Set the indices $\mathbf{i}^{m}$ for each data window, $m \in \{ 1 : M \}$ \;
	Set the indices $\mathbf{j}^{m}$ for each source window, $m \in \{ 1 : M \}$ \;
	Set the constant depth $z_{0} + \Delta z_{0}$ for all equivalent sources \;
	Compute the vector $\mathbf{a}'$ associated with the desired potential-field transformation \;
	Compute the matrix $\mathbf{G}'$ \;
	Compute $\left( \mathbf{G}' \mathbf{a}' \right)^{\top} 
	\left[ \mathbf{G}' \, \left( \mathbf{G}' \right)^{\top} + \mu \, \mathbf{I}_{D'} \right]^{-1}$ \;
	$m = 1$ \;
	\While{$m < M$}{
		Compute $t^{m}_{c}$ (equation \ref{eq:transformed-field-tmc-LS89}) \;
		$m \gets m + 1$ \;
	}
	\caption{Generic pseudo-code for the method proposed by \cite{leao-silva1989}.}
	\label{alg:LS89}
\end{algorithm}

\cite{soler-uieda2021} generalized the method proposed by \cite{leao-silva1989} for irregularly spaced data on an undulating surface.
A direct consequence of this generalization is that a different submatrix $\mathbf{G}^{m} \equiv \mathbf{G}[\mathbf{i}^{m}, \mathbf{j}^{m}]$ 
(equation \ref{eq:predicted-data-window-m}) must be computed for each window.
Differently from \cite{leao-silva1989}, \cite{soler-uieda2021} store the computed $\tilde{\mathbf{p}}^{m}$ for all windows and
subsequently use them to obtain a desired potential-field transformation (equation \ref{eq:transformation}) as the superposed
effect of all windows.
The estimated $\tilde{\mathbf{p}}^{m}$ for all windows are combined to form a single $P \times 1$ vector $\tilde{\mathbf{p}}$,
which is an estimate for original parameter vector $\mathbf{p}$ (equation \ref{eq:predicted-data-vector}).
For each data window, \cite{soler-uieda2021} solve an overdetermined problem (equation \ref{eq:delta-q-tilde-overdetermined}) 
for $\tilde{\mathbf{p}}^{m}$ by using 
$\mathbf{H} = \mathbf{W}_{p} = \mathbf{I}_{P^{m}}$ (equations \ref{eq:reparameterization} and \ref{eq:weighting-matrix-q}),
$\mathbf{W}^{m}_{d}$ (equation \ref{eq:function-Phi}) equal to a diagonal matrix of weights for the data inside the $m$-th window
and $\bar{p} = \mathbf{0}$ (equation \ref{eq:reparameterization-reference}), so that
\begin{equation}
	\left[ \left( \mathbf{G}^{m} \right)^{\top} \mathbf{W}^{m}_{d} \, \mathbf{G}^{m} + 
	\mu \, \mathbf{I}_{P'} \right] 
	\tilde{\mathbf{p}}^{m} = 
	\left( \mathbf{G}^{m} \right)^{\top} \mathbf{W}^{m}_{d} \: 
	\mathbf{d}^{m} \: .
	\label{eq:p-tilde-m-SU21}
\end{equation}
The overall steps of their method are defined by the Algorithm \ref{alg:SU21}.
Note that Algorithm \ref{alg:SU21} starts with a residuals vector $\mathbf{r}$ that is iteratively updated.
At each iteration, the potential field predicted a source window is computed at all observation points and removed from the
residuals vector $\mathbf{r}$.

\begin{algorithm}
	%\SetAlgoLined
	\Input{}
	Set the indices $\mathbf{i}^{m}$ for each data window, $m \in \{ 1 : M \}$ \;
	Set the indices $\mathbf{j}^{m}$ for each source window, $m \in \{ 1 : M \}$ \;
	Set the depth of all equivalent sources \;
	Set a $D \times 1$ residuals vector $\mathbf{r} = \mathbf{d}$ \;
	Set a $P \times 1$ vector $\tilde{\mathbf{p}} = \mathbf{0}$ \;
	$m = 1$ \;
	\While{$m < M$}{
		Set the matrix $\mathbf{W}^{m}_{d}$ \;	
		Compute the matrix $\mathbf{G}^{m}$ \;
		Compute $\tilde{\mathbf{p}}^{m}$ (equation \ref{eq:p-tilde-m-SU21}) \;
		$\tilde{\mathbf{p}}[\mathbf{j}^{m}] \gets \tilde{\mathbf{p}}[\mathbf{j}^{m}] + \tilde{\mathbf{p}}^{m}$ \;
		$\mathbf{r} \gets \mathbf{r} - \mathbf{G}[:,\mathbf{j}^{m}] \, \tilde{\mathbf{p}}^{m}$ \;
		$m \gets m + 1$ \;
	}
	\caption{Generic pseudo-code for the method proposed by \cite{soler-uieda2021}.}
	\label{alg:SU21}
\end{algorithm}

\subsection{Column update}

\cite{cordell1992} proposed a computational strategy that was later used by \cite{guspi-novara2009} and relies on
first defining one equivalent source located right below each observed data $d_{i}$, $i \in \{1:D\}$, at a vertical
coordinate $z_{i} + \Delta z_{i}$, where $\Delta z_{i}$ is proportional to the distance from the $i$-th observation point 
$(x_{i}, y_{i}, z_{i})$ to its closest neighbor.
The second step consists in updating the physical property $p_{j}$ of a given equivalent source, $j \in \{1:D\}$ and 
remove its predicted potential field from the observed data vector $\mathbf{d}$, producing a residuals vector $\mathbf{r}$. 
Then, the same procedure is repeated for other sources with the purpose of iteratively updating $\mathbf{r}$ and the
$D \times 1$ parameter vector $\mathbf{p}$ containing the physical property of all equivalent sources.
At the end, the algorithm produces an estimate $\tilde{\mathbf{p}}$ for the parameter vector yielding a predicted
potential field $\mathbf{f}$ (equation \ref{eq:predicted-data-vector}) satisfactorily fitting the observed data
$\mathbf{d}$ according to a given criterion.
Note that the method proposed by \cite{cordell1992} iteratively solves the linear $\mathbf{G} \tilde{\mathbf{p}} \approx \mathbf{d}$
with a $D \times D$ matrix $\mathbf{G}$. At each iteration, only a single column of $\mathbf{G}$ (equation \ref{eq:predicted-data-vector}) 
is used.
An advantage of this \textit{column-update approach} is that the full matrix $\mathbf{G}$ is never stored.

Algorithm \ref{alg:C92} delineates the \citeauthor{cordell1992}'s method.
Note that a single column $\mathbf{G}[:, i_{\mathtt{max}}]$ of the $D \times D$ matrix $\mathbf{G}$ (equation \ref{eq:predicted-data-vector})
is used per iteration, where $i_{\mathtt{max}}$ is the index of the maximum absolute value in $\mathbf{r}$.
As pointed out by \cite{cordell1992}, the method does not necessarily decrease monotonically along the iterations.
Besides, the method may not converge depending on how the vertical distances $\Delta z_{i}$, $i \in \{1:D\}$, 
controlling the depths of the equivalent sources are set.
According to \cite{cordell1992}, the maximum absolute value $r_{\mathtt{max}}$ in $\mathbf{r}$ decreases robustly at the beginning 
and oscillates within a narrowing envelope for the subsequent iterations.

\begin{algorithm}
	%\SetAlgoLined
	\Input{}
	Compute a $D \times 1$ vector $\boldsymbol{\Delta}\mathbf{z}$ whose $i$-th element $\Delta z_{i}$ 
	is a vertical distance controlling the depth of the $i$-th equivalent source, $i \in \{1:D\}$ \;
	Set a tolerance $\epsilon$ \;
	Set a maximum number of iteration $\mathtt{ITMAX}$ \;
	Set a $D \times 1$ residuals vector $\mathbf{r} = \mathbf{d}$ \;
	Set a $D \times 1$ vector $\tilde{\mathbf{p}} = \mathbf{0}$ \;
	Define the maximum absolute value $r_{\mathtt{max}}$ in $\mathbf{r}$ \;
	$m = 1$ \;
	\While{$(r_{\mathtt{max}} > \epsilon)$ {\bf and} $(m < \mathtt{ITMAX})$}{
		Define the coordinates $(x_{\mathtt{max}}, y_{\mathtt{max}}, z_{\mathtt{max}})$ and index $i_{\mathtt{max}}$ of the observation point associated with $r_{\mathtt{max}}$ \;
		$\tilde{\mathbf{p}}[i_{\mathtt{max}}] \gets \tilde{\mathbf{p}}[i_{\mathtt{max}}] + \left( a_{\mathtt{max}} \, \boldsymbol{\Delta}\mathbf{z}[i_{\mathtt{max}}] \right)$ \;
		$\mathbf{r} \gets \mathbf{r} - \left( \mathbf{G}[:, i_{\mathtt{max}}] \, \tilde{\mathbf{p}}[i_{\mathtt{max}}] \right)$ \;
		Define the new $r_{\mathtt{max}}$ in $\mathbf{r}$ \;
		$m \gets m + 1$ \;
	}
	\caption{Generic pseudo-code for the method proposed by \cite{cordell1992}.}
	\label{alg:C92}
\end{algorithm}

\subsection{Row update}

\cite{mendonca-silva1994} proposes an algebraic reconstruction technique (ART) \cite[e.g.,][p. 58]{sluis-vorst1987}
to estimate a parameter vector $\tilde{\mathbf{p}}$ for a regular grid of $P$ equivalent sources on a horizontal plane $z_{0}$.
Such methods iterate on the linear system rows to estimate corrections for the parameter vector,
which may substantially save computer time and memory required to compute and store the full linear system matrix
along the iterations.
The convergence of such \textit{row-update methods} depends on the linear system condition.
The main advantage of such methods is not computing and storing the full linear system matrix, but iteratively using 
its rows.
The particular ART method proposed by \cite{mendonca-silva1994} considers that
\begin{equation}
	\mathbf{d} = \begin{bmatrix}
		\mathbf{d}_{e} \\ \mathbf{d}_{r}
	\end{bmatrix} \: , \quad 
	\mathbf{G} = \begin{bmatrix}
		\mathbf{G}_{e} \\ \mathbf{R}_{r}
	\end{bmatrix} \: ,
	\label{eq:partitioned-d-G-MS94}
\end{equation}
where $\mathbf{d}_{e}$ and $\mathbf{d}_{r}$ are $D_{e} \times 1$ and $D_{r} \times 1$ vectors and
$\mathbf{G}_{e}$ and $\mathbf{G}_{r}$ are $D_{e} \times P$ and $D_{r} \times P$ matrices, respectively.
\cite{mendonca-silva1994} designate $\mathbf{d}_{e}$ and $\mathbf{d}_{r}$ as, respectively, \textit{equivalent} and \textit{redundant} data.
With the exception of a normalization strategy, \cite{mendonca-silva1994} calculate a $P \times 1$ estimated parameter vector $\tilde{\mathbf{p}}$ 
by solving an underdetermined problem (equation \ref{eq:delta-q-tilde-underdetermined}) involving only the equivalent data $\mathbf{d}_{e}$ 
(equation \ref{eq:partitioned-d-G-MS94})
for the particular case in which $\mathbf{H} = \mathbf{W}_{p} = \mathbf{I}_{P}$ (equations \ref{eq:reparameterization} and \ref{eq:weighting-matrix-q}),
$\mathbf{W}_{d} = \mathbf{I}_{D_{e}}$ (equation \ref{eq:function-Phi}) and $\bar{p} = \mathbf{0}$ (equation \ref{eq:reparameterization-reference}), 
which results in
\begin{equation}
	\begin{split}
		\left(\mathbf{F} + \mu \, \mathbf{I}_{D_{e}} \right) \mathbf{u} = \mathbf{d}_{e} \\
		\tilde{\mathbf{p}} = \mathbf{G}_{e}^{\top} \mathbf{u}
	\end{split} \quad ,
	\label{eq:p-tilde-MS94}
\end{equation}
where $\mathbf{F}$ is a $P \times P$ matrix that replaces $\mathbf{G}_{e} \, \mathbf{G}_{e}^{\top}$.
\cite{mendonca-silva1994} presume that the estimated parameter vector $\tilde{\mathbf{p}}$ obtained from equation \ref{eq:p-tilde-MS94}
leads to a $D_{r} \times 1$ residuals vector
\begin{equation}
	\mathbf{r} = \mathbf{d}_{r} - \mathbf{G}_{r} \tilde{\mathbf{p}} 
	\label{eq:residuals-MS94}
\end{equation}
having a maximum absolute value $r_{\mathtt{max}} \le \epsilon$, where $\epsilon$ is a predefined tolerance.

The overall method of \cite{mendonca-silva1994} is defined by Algorithm \ref{alg:MS94}.
It is important noting that the number $D_{e}$ of equivalent data in $\mathbf{d}_{e}$ increases by one per iteration,
which means that the order of the linear system in equation $\ref{eq:p-tilde-MS94}$ also increases by one at each iteration.
Those authors also propose a computational strategy based on Cholesky factorization \cite[e.g.,][p. 163]{golub-vanloan2013}
for efficiently updating 
$\left(\mathbf{F} + \mu \, \mathbf{I}_{D_{e}} \right)$ at a given iteration (line 16 in Algorithm \ref{alg:MS94}) 
by computing only its new elements with respect to those computed in the previous iteration.

\begin{algorithm}
	%\SetAlgoLined
	\Input{}
	Set a regular grid of $P$ equivalent sources at a horizontal plane $z_{0}$ \;
	Set a tolerance $\epsilon$ \;
	Set a $D \times 1$ residuals vector $\mathbf{r} = \mathbf{d}$ \;
	Define the maximum absolute value $r_{\mathtt{max}}$ in $\mathbf{r}$ \; 
	Define the index $i_{\mathtt{max}}$ of $r_{\mathtt{max}}$ \;
	Define the list of indices $\mathbf{i}_{r}$ of the remaining data in $\mathbf{r}$ \;
	Define $\mathbf{d}_{e} = \mathbf{d}[i_{\mathtt{max}}] $ \;
	Compute $\left(\mathbf{F} + \mu \, \mathbf{I}_{D_{e}} \right)$ and $\mathbf{G}_{e}$ \;
	Compute $\tilde{\mathbf{p}}$ (equation \ref{eq:p-tilde-MS94}) \;
	Compute $\mathbf{r} = \mathbf{d}[\mathbf{i}_{r}] - \mathbf{G}[\mathbf{i}_{r}, :] \, \tilde{\mathbf{p}}$ \;
	Define the maximum absolute value $r_{\mathtt{max}}$ in $\mathbf{r}$ \; 
	\While{$(r_{\mathtt{max}} > \epsilon)$}{
		Define the index $i_{\mathtt{max}}$ of $r_{\mathtt{max}}$ \;
		Define the list of indices $\mathbf{i}_{r}$ of the remaining elements in $\mathbf{r}$ \;
		$\mathbf{d}_{e} \gets \begin{bmatrix} \mathbf{d}_{e} \\ \mathbf{d}[i_{\mathtt{max}}] \end{bmatrix}$ \;
		Update $\left(\mathbf{F} + \mu \, \mathbf{I}_{D_{e}} \right)$ and $\mathbf{G}_{e}$ \;
		Compute $\tilde{\mathbf{p}}$ (equation \ref{eq:p-tilde-MS94}) \;
		Compute $\mathbf{r} = \mathbf{d}[\mathbf{i}_{r}] - \mathbf{G}[\mathbf{i}_{r}, :] \, \tilde{\mathbf{p}}$ \;
		Define the maximum absolute value $r_{\mathtt{max}}$ in $\mathbf{r}$ \; 
	}
	\caption{Generic pseudo-code for the method proposed by \cite{mendonca-silva1994}.}
	\label{alg:MS94}
\end{algorithm}

\subsection{Reparameterization}

Another approach for improving the computational performance of equivalent-layer technique consists in 
setting a $P \times Q$ reparameterization matrix $\mathbf{H}$ (equation \ref{eq:reparameterization})
with $Q << P$. 
In this case, an estimate $\tilde{\mathbf{q}}$ for the reparameterized parameter vector $\mathbf{q}$
is obtained and subsequently used to obtain an estimate $\tilde{\mathbf{p}}$ 
for the parameter vector $\mathbf{p}$ (equation \ref{eq:predicted-data-vector}) by using equation
\ref{eq:reparameterization}. 
The idea of this \textit{reparameterization approach} is solving an appreciably smaller linear inverse problem for 
$\tilde{\mathbf{q}}$ than that for obtaining an estimate $\tilde{\mathbf{p}}$ for the original parameter
vector (equation \ref{eq:predicted-data-vector}).

\cite{oliveirajr-etal2013} have used this approach to describe the physical property distribution on the
equivalent layer in terms of piecewise bivariate polynomials.
Specifically, their method consists in splitting a regular grid of equivalent sources into 
source windows inside which the physical-property distribution is described by bivariate polynomial 
functions. The key aspect of their method relies on the fact that the total number of coefficients 
required to define the bivariate polynomials is considerably smaller than the original number of equivalent sources. 
Hence, they formulate a linear inverse problem for estimating the polynomial coefficients and use them later
to compute the physical property distribution on the equivalent layer. 

The method proposed by \cite{oliveirajr-etal2013} consists in solving an overdetermined problem 
(equation \ref{eq:delta-q-tilde-overdetermined}) for estimating the polynomial coefficients 
$\tilde{\mathbf{q}}$ with $\mathbf{W}_{d} = \mathbf{I}_{D}$ (equation \ref{eq:function-Phi}) and
$\bar{q} = \mathbf{0}$ (equation \ref{eq:reparameterization-reference}), so that
\begin{equation}
	\left( \mathbf{H}^{\top} \mathbf{G}^{\top} \mathbf{G} \, \mathbf{H} + 
	\mu \, \mathbf{W}_{q} \right) 
	\tilde{\mathbf{q}} = 
	\mathbf{H}^{\top} \mathbf{G}^{\top} \: \mathbf{d} \: ,
	\label{eq:q-tilde-OBU13}
\end{equation}
where $\mathbf{W}_{q}$ is defined by equation \ref{eq:weighting-matrix-q}, with a matrix $\mathbf{W}_{p}$
representing the zeroth- and first-order Tikhonov regularization \cite[e.g.,][p. 103]{aster_etal2019}.
Note that, in this case, the prior information is defined for the original parameter vector $\mathbf{p}$
and then transformed to the $\mathbf{q}$ space.
Another characteristic of their method is that it is valid for processing irregularly-spaced data
on an undulating surface.

\cite{mendonca-2020} also proposed a reparameterization approach for the equivalent-layer technique.
Their approach, however, consists in setting $\mathbf{H}$ as a truncated singular value decomposition
(SVD) \cite[e.g.,][p. 55]{aster_etal2019} of the observed potential field. 
Differently from \cite{oliveirajr-etal2013}, however, the method of \cite{mendonca-2020} requires 
a regular grid of potential-field data on horizontal plane.
Another difference is that these authors uses $\mathbf{W}_{q} = \mathbf{I}_{Q}$ (equation \ref{eq:weighting-matrix-q}),
which means that the regularization is defined directly in the $\mathbf{q}$ space.

Similarly to \cite{oliveirajr-etal2013} and \cite{mendonca-2020}, \cite{barnes-lumley_2011} also proposed a computationally
efficient method for equivalent-layer technique based on reparameterization. 
A key difference, however, is that \cite{barnes-lumley_2011} did not set a $P \times Q$ reparameterization matrix $\mathbf{H}$
(equation \ref{eq:reparameterization}) with $Q << P$. 
Instead, their method uses a matrix $\mathbf{H}$ with $Q \approx 1.7 \, P$. 
Their central idea is setting a reparameterization scheme that groups distant equivalent sources into blocks by
using a bisection process. 
This scheme leads to a quadtree representation of the physical-property distribution on the equivalent layer, 
so that matrix $\mathbf{G}\mathbf{H}$ (equation \ref{eq:predicted-data-vetor-reparameterized}) is notably sparse.
\cite{barnes-lumley_2011} explore this sparsity in solving the overdetermined problem for $\tilde{\mathbf{q}}$
(equation \ref{eq:q-tilde-OBU13}) via conjugate-gradient algorithm \cite[e.g.,][sec. 11.3]{golub-vanloan2013}.

PAREI AQUI


\subsection{Wavelet compression}

\cite{li-oldenburg_2010}

\subsection{Iterative methods using the original $\mathbf{G}$}

Ideia: descrever o conjugate gradient e depois as diferenças dos outros métodos iterativos

\cite{xia-sprowl1991}

\cite{xia-etal1993}

\cite{siqueira-etal2017}

\cite{jirigalatu-ebbing2019}


\subsection{Discrete convolution}

\cite{takahashi-etal2020}

\cite{takahashi-etal2022}

\section{TEXTO ANTIGO}

\subsubsection{The wavelet compression and lower-dimensional subspace}

For large data sets, the  sensitivity matrix $\mathbf{A}$ (equation \ref{eq:predicted-data-vector}) is a drawback in applying  
the equivalent-layer technique because it is a large and dense matrix.

\cite{li-oldenburg2010} transformed a large and full sensitivity matrix into a sparse one by using fast wavelet transforms.
In the wavelet domain, \cite{li-oldenburg2010} applyied a 2D wavelet transform to each row and column of the original sensitivity matrix $\mathbf{A}$ to expand it in the wavelet bases. 
This operation can be done by premultiplying the original sensitivity matrix $\mathbf{A}$ by a 
matrix representing the 2D wavelet transform $\mathbf{W_2}$ and then the resulting is postmultiplied by 
the transpose of $\mathbf{W_2}$ (i.e., $\mathbf{W_2}^{\top}$).
\begin{equation}
	\mathbf{\tilde{A}} = \mathbf{W_2} \: \mathbf{A}  \: \mathbf{W_2}^{\top} \:,
	\label{eq:A_li_oldenburg}
\end{equation}
where $\mathbf{\tilde{A}}$ is the expanded original sensitivity matrix in the wavelet bases with many elements zero or close to zero.
Next, the matrix $\mathbf{\tilde{A}}$ is replaced by its sparse version $\mathbf{\tilde{A}_{s}}$ 
in the wavelet domain which in turn is obtained by retaining only 
the large elements of the $\mathbf{\tilde{A}}$.
Thus, the elements of  $\mathbf{\tilde{A}}$ whose  amplitudes fall below a relative threshold are discarded.
In \cite{li-oldenburg2010}, the original sensitivity matrix $\mathbf{A}$ is high compressed resulting in 
a sparce matrix $\mathbf{\tilde{A}_{s}}$ with a few percent of nonzero elements and 
the the inverse problem is solved in the wavelet domain by using $\mathbf{\tilde{A}_{s}}$ and 
a incomplete conjugate gradient least squares, without an explicit regularization parameter and 
a limited number of iterations.
The solution is obtained by solving the following linear system
\begin{equation}
	 \mathbf{\tilde{A}_{L}}^{\top} \: \mathbf{\tilde{A}_{L}} \: \mathbf{\tilde{p}_{L}}^{\ast} \: =  
	 \mathbf{\tilde{A}_{L}}^{\top} \: \mathbf{\tilde{d}}^{o} \: ,
	\label{eq:linear_system_li_oldenburg}
\end{equation}
where $\mathbf{\tilde{p}_{L}}^{\ast}$ is obtained by solving the linear system given by equation
\ref{eq:linear_system_li_oldenburg},
\begin{subequations}
\begin{eqnarray}
\mathbf{\tilde{A}_{L}} &=& \mathbf{\tilde{A}_{s}} \: \mathbf{\tilde{L}^{-1}}, \\
\mathbf{\tilde{p}_{L}} &=& \mathbf{\tilde{L}} \mathbf{\tilde{p}}, \\
\mathbf{\tilde{d}}^{o} &=&  \mathbf{W_2} \: \mathbf{d}^{o}, 
\end{eqnarray}
\label{eq:pl_system_li_oldenburg}
\end{subequations}
where $\mathbf{\tilde{L}}$ is a diagonal and invertible weighting matrix representing the finite-difference approximation in the wavelet domain. 
Finally, the distribution over the equivalent layer in the space domain $\mathbf{p}$ is obtained by applying an inverse wavelet transform in two steps, i.e.:
\begin{equation}
	\mathbf{\tilde{p}}  = \mathbf{\tilde{L}}^{-1} \:  \mathbf{\tilde{p}_{L}}^{\ast} \:,
	\label{eq:ptil_li_oldenburg}
\end{equation}
and
\begin{equation}
	 \mathbf{p}  = \mathbf{W_2} \: \mathbf{\tilde{p}} \:.
	\label{eq:p_li_oldenburg}
\end{equation}
Although the data misfit quantifying the difference between the observed and  predicted data 
by the equivalent source is calculated in the wavelet domain, we understand that the desired transformation
is calculated via equation \ref{eq:t_data} which  uses a full matrix of Green's functions $\mathbf{T}$.

\cite{li-oldenburg2010} used the equivalent-layer technique with a wavelet compression to perform 
an  upward continuation of total-field anomaly between uneven surfaces.
For regularly spaced grid of data,  \cite{li-oldenburg2010} reported that high compression ratios 
are achived with insignificant loss of accuracy.
As compared to the upward-continued total-field anomaly by equivalent layer using the dense matrix,
\citeauthor{li-oldenburg2010}'s (\citeyear{li-oldenburg2010}) approach, using the Daubechies wavelet,
decreased CPU (central processing unit) time by up to two orders of magnitude.

\cite{mendoncca2020} overcame the solution of intractable large-scale equivalent-layer problem by using the subspace method (e.g., 
\citeauthor{skilling-bryan1984}, \citeyear{skilling-bryan1984};
\citeauthor{kennett1988}, \citeyear{kennett1988};
\citeauthor{oldenburg1993}, \citeyear{oldenburg1993};  
\citeauthor{barbosa-etal1997}, \citeyear{barbosa-etal1997}).
The subspace method  reduces the dimension of the linear system of equations to be solved. 
Given a higher-dimensional space (e.g., $M$-dimensional model space, 
$\mathbb{R}^{M}$), there exists many lower-dimensional subspaces 
(e.g., $Q$-dimensional subspace) of $\mathbb{R}^{M}$.
The linear inverse problem related to the equivalent-layer technique 
consists in finding an $M$-dimension parameter vector $\mathbf{p} \: \in \mathbb{R}^{M}$ which adequately fits the potential-field data.
The subspace method looks for a parameter vector who lies in a $Q$-dimensional subspace of $\mathbb{R}^{M}$ which, in turn, is spanned by a set of $Q$ vectors 
$\mathbf{v}_i = 1, ..., Q$, where $\mathbf{v}_i \in \mathbb{R}^{M}$
In matrix notation,  the parameter vector in the subspace method 
can be written as  
\begin{equation}
	\mathbf{p} = \mathbf{V} \: \boldmath{\alpha}  \:,
	\label{eq:p_subspace}
\end{equation}
where $\mathbf{V}$ is an $M \times Q$ matrix whose columns 
$\mathbf{v}_i = 1, ..., Q$ form a basis vectors for a subspace $Q$ of 
$\mathbb{R}^{M}$.
In equation \ref{eq:p_subspace}, the parameter vector $\mathbf{p}$ 
is defined as a linear combination in the space spanned by $Q$ basis vectors 
$\mathbf{v}_i = 1, ..., Q$  and {\boldmath$\alpha$}  is a $Q$-dimensional 
unknown vector to be determined.
The main advantage of the subspace method is that the linear system of 
$M$ equations in $M$ unknowns to be originally solved 
is reduced to a new linear system of $Q$ equations in $Q$ unknowns
which requires much less computational effort since $Q << M$, i.e.:
\begin{equation}
	 \mathbf{V}^{\top} \:  \mathbf{A}^{\top} \: \mathbf{A} \:  \mathbf{V}  \: \boldmath{\alpha}^{\ast} \: = \:
	 \mathbf{V}^{\top}  \: \mathbf{d}^{o} \:.
	\label{eq:linear_system_subspace}
\end{equation}
To avoid the storage of matrices $\mathbf{A}$ and $\mathbf{V}$,  \cite{mendoncca2020} 
evaluates an element of the matrix  $\mathbf{A} \mathbf{V}$ by
calculating the dot product between the row of matrix $\mathbf{A}$ and the column of the matrix 
$\mathbf{B}$. 
After estimating {\boldmath$\alpha^{\ast}$} (equation \ref{eq:linear_system_subspace}) 
belonging to a $Q$-dimensional subspace of $\mathbb{R}^{M}$,
the distribution over the equivalent layer  $\mathbf{p}$ in the $\mathbb{R}^{M}$ is obtained by applying equation \ref{eq:p_subspace}.
The choice of the $Q$ basis vectors $\mathbf{v}_i = 1, ..., Q$ 
(equation \ref{eq:p_subspace}) in the subspace method is not strict.
\cite{mendoncca2020}, for example, chose the eigenvectors yielded by 
applying the singular value decomposition of the matrix containing the gridded data set.
The number of eigenvectors used to form basis vectors will depend on 
the singular values. 

The proposed subspace method for solving large-scale equivalent-layer problem by \cite{mendoncca2020} was applied to  estimate the mass excess or deficiency caused by causative gravity sources.


\subsubsection{The quadtree discretization}

To make the equivalent-layer technique tractable, \cite{barnes-lumley2011} also transformed the dense sensitivity matrix $\mathbf{A}$ (equation \ref{eq:predicted-data-vector}) into a sparse matrix.
In \cite{barnes-lumley2011}, a sparce version of the sensitivity matrix is achived by grouping equivalent sources (e.g., they used prisms) distant from an observation point together to form a larger prism 
or larger block.
Each larger block has averaged physical properties and averaged top- and bottom-surfaces of the grouped smaller prisms (equivalent sources) that are encompassed by the larger block.
The authors called it the 'larger averaged block' and the essence of their method is the reduction 
in the number of equivalent sources, which means a reduction in the number of parameters to be estimated 
implying in model dimension reduction.

The key of the \citeauthor{barnes-lumley2011}'s (\citeyear{barnes-lumley2011}) method is the 
algorithm for deciding how to group the smaller prisms.
In practice, these authors used a recursive bisection process that results in a quadtree discretization  
of the equivalent-layer model. 

By using the quadtree discretization, \cite{barnes-lumley2011} were able to jointly process multiple components of airborne gravity-gradient data using a single layer of equivalent sources. 
To our knowledge, \cite{barnes-lumley2011} are the pioneers on processing 
full-tensor gravity-gradient data jointly.
In addition to computational feasibility,  \citeauthor{barnes-lumley2011}'s (\citeyear{barnes-lumley2011}) method reduces low-frequency noise and can also remove the drift in time-domain from the survey data. 
Those authors stressed that the $G_{zz}-$component calculated through the single estimated equivalent-layer model projected on a grid at a constant elevation by inverting full gravity-gradient data
has the low-frequency error reduced by a factor of 2.4 as compared to the inversion 
of an individual component of the gravity-gradient data.



\subsubsection{The reparametrization of the equivalent layer}

\cite{oliveirajr-etal2013} reparametrized the whole equivalent-layer model by 
a piecewise  bivariate-polynomial function defined on a set of $Q$ equivalent-source windows. 
In \citeauthor{oliveirajr-etal2013}'s (\citeyear{oliveirajr-etal2013}) approach, 
named polynomial equivalent layer (PEL), the parameter vector within the $k$th equivalent-source window 
$\mathbf{p}^{k}$  can be written in matrix notation as  
\begin{equation}
	\mathbf{p}^{k} = \mathbf{B}^{k} \: \mathbf{c}^{k} \:,  \:\:\:\:\:\:\: k \: = \: 1 \:... \: Q \:, 
	\label{eq:p_pel}
\end{equation}
where $\mathbf{p}^{k}$ is an $M_w$-dimensional vector containing the physical-property distribution
within the $k$th equivalent-source window, 
$\mathbf{c}^{k}$ is a $P$-dimensional vector whose $l$th element is the $l$th
coefficient of the $\alpha$th-order polynomial function and
$\mathbf{B}^{k}$ is an $M_w \times P$ matrix containing the first-order derivative of the 
$\alpha$th-order polynomial function with respect to one of the $P$ coefficients.

By using a regularized potential-field inversion, \cite{oliveirajr-etal2013} estimates the polynomial coefficients for each equivalent-source window by solving the following linear system
\begin{equation}
	\left( \mathbf{B}^{\top} \: \mathbf{A}^{\top} \: \mathbf{A} \: \mathbf{B}   \: +
	 \: \mu \mathbf{I} \right) \:  \mathbf{c}^{\ast}
	 \: = \: \mathbf{B}^{\top} \:  \mathbf{A}^{\top} \: \mathbf{d}^{o} \: ,
	\label{eq:linear_system_pel}
\end{equation}
where  $\mu$ is a regularizing parameter, 
$\mathbf{c}^{\ast}$ is an estimated $H$-dimensional vector containing all coefficients describing all polynomial functions within  all equivalent-source windows which compose the entire equivalent layer,
$\mathbf{I}$ is  an identity matrix of order $H (H = P \dot Q) $ and
$\mathbf{B}$ is an $M \times H$  block diagonal matrix such that the main-diagonal blocks 
are $\mathbf{B}^{k}$ matrices (equation \ref{eq:p_pel}) and all off-diagonal blocks are zero matrices.
For ease of the explanation of equation \ref{eq:linear_system_pel}, we keep only the zeroth-order Tikhonov regularization and omitting the first-order Tikhonov regularization \citep{aster2018parameter} 
which was also used  by \cite{oliveirajr-etal2013}.

The main advantage of the PEL is solve $H$-dimensional system of equations 
(equation \ref{eq:linear_system_pel}), where $H$ totalizes the number of polynomial coefficients
composing all equivalent-source windows,  requiring a lower computational effort since $H <<< N$.
To avoid the storage of matrices $\mathbf{A}$ and $\mathbf{B}$,  \cite{oliveirajr-etal2013}
evaluate an element of the matrix $\mathbf{A} \mathbf{B} $ by calculating the 
dot product between the row of matrix $\mathbf{A}$ and the column of the matrix $\mathbf{B}$. 
After estimating all polynomial coefficients of all windows, the estimated coefficients 
($\mathbf{c}^{\ast}$ in equation \ref{eq:linear_system_pel})
are transformed into a single physical-property distribution encompassing the entire equivalent layer. 

As stated by \cite{oliveirajr-etal2013}, the computational efficiency of PEL approach
stems from the fact that the total number of polynomial coefficients $H$ required to depict the 
physical-property distribution within the equivalent layer is generally much smaller than the number of equivalent sources. 
Consequently, this leads to a considerably smaller linear system that needs to be solved.
Hence, the main strategy of polynomial equivalent layer  is the model dimension reduction.

The polynomial equivalent layer was applied to perform upward continuations of gravity and magnetic data 
and reduction to the pole of magnetic data.


\subsubsection{The iterative scheme without solving a linear system}

There exists a class of methods that iteratively estimate the distribution of physical properties 
within an equivalent layer without the need to solve linear systems. 
The method initially introduced by \cite{cordell1992} and later expanded upon by \cite{guspi-novara2009} updates the physical property of sources, located beneath each potential-field data, by removing the maximum residual between the observed and fitted data. 
In addition, \cite{xia-sprowl1991} and \cite{xia-etal1993} have developed efficient iterative algorithms 
for updating the distribution of physical properties within the equivalent layer in the wavenumber and space domains, respectively.
Specifically, in \citeauthor{xia-sprowl1991}'s (\citeyear{xia-sprowl1991}) method the physical-property distribution is updated by using the ratio between the squared depth to the equivalent source and the gravitational constant multiplied by the residual between the observed and predicted observation at the measurement station. 
Neither of these methods solve linear systems.

Following this class of methods of iterative equivalent-layer technique that does not solve linear systems, \cite{siqueira-etal2017} developed a fast iterative equivalent-layer technique for processing gravity data
in which the sensitivity matrix $\mathbf{A}$ (equation \ref{eq:predicted-data-vector}) 
is replaced by a diagonal matrix $ N \times N$, i.e.:
\begin{equation}
	\mathbf{\tilde{\tilde{A}}} = 2\: \pi \: \gamma \: \mathbf{{\Delta S}^{-1}}  \: ,
	\label{eq:A_siqueira}
\end{equation}
where $\gamma$ is Newton's gravitational constant  and 
$\mathbf{{\Delta S}^{-1}}$ is a diagonal matrix of order $N$ whose diagonal elements ${\Delta s}_{i}$, $i = 1, ..., N$  are the element of area centered at the $i$th horizontal coordinates of the $i$th observation point.
The physical foundations of \citeauthor{siqueira-etal2017}'s (\citeyear{siqueira-etal2017}) method  
rely on two constraints: i) the excess of mass; and ii)  the positive correlation between the gravity observations and the mass distribution over the equivalent layer.

Although \citeauthor{siqueira-etal2017}'s (\citeyear{siqueira-etal2017}) method  does not solve any 
linear system of equations, it can be theoretically explained by solving the following linear system
at the $k$th iteration:
\begin{equation}
	\mathbf{\tilde{\tilde{A}}}^{\top} \: \mathbf{\tilde{\tilde{A}}} {{\mathbf{\Delta \: \hat{p}}} }^{k}
	 \: = \:  \mathbf{\tilde{\tilde{A}}}^{\top} \: \mathbf{r}^{k} \: ,
	\label{eq:linear_system_siqueira}
\end{equation}
where $ \mathbf{r}^{k} $ is an $N$-dimensional residual vector whose $i$th element is calculated by subtracting the  $i$th observed data $d^{o}_{i}$ from the  $i$th fitted data $d^{k}_{i}$ at the $k$th iteration, i.e.,
\begin{equation}
	r^{k}_{i} =  d^{o}_{i} \: - \: d^{k}_{i} \:.
	\label{eq:r_siqueira}
\end{equation}
and ${{\mathbf{\Delta \: \hat{p}}} }^{k}$ is an estimated $N$-dimensional vector of parameter correction.

Because $\mathbf{\tilde{\tilde{A}}}$,  in equation \ref{eq:linear_system_siqueira}, is a diagonal matrix 
(equation \ref{eq:A_siqueira}), the parameter correction estimate is directly calculated  without solving system of linear equations, and thus, an $i$th element of ${{\mathbf{\Delta \: \hat{p}}} }^{k}$  
is directly calculated by
\begin{equation}
	{{\Delta \hat{p}}^{k}}_{i} = \frac{{\Delta s}_{i} \: r^{k}_{i} } 
	{2\: \pi \: \gamma}   \: .
	\label{eq:deltap_siqueira}
\end{equation}
The mass distribution over the equivalent layer is updated by: 
\begin{equation}
	{\hat{p}}^{k+1}_{i} = {\hat{p}}^{k}_{i} \: + \: {{\Delta \hat{p}}^{k}}_{i} \: .
	\label{eq:p_siqueira}
\end{equation}
\citeauthor{siqueira-etal2017}'s (\citeyear{siqueira-etal2017}) method starts from a mass distribution 
on the equivalent layer, whose $i$th mass $p^{o}_{i}$ is proportional to the $i$th 
observed data $d^{o}_{i}$, i.e.,
\begin{equation}
	p^{o}_{i} = \frac{{\Delta s}_{i} \: d^{o}_{i} }{2\: \pi \: \gamma}   \: .
	\label{eq:po_siqueira}
\end{equation}

\cite{siqueira-etal2017} applied their fast iterative equivalent-layer 
technique to interpolate, calculate the horizontal components, and
continue upward (or downward) gravity data.

For jointly process two gravity gradient components, 
\cite{jirigalatu-ebbing2019} used the Gauss-FFT for forward calculation of potential fields in the wavenumber domain combined with Landweber's iteration coupled with a mask matrix $\mathbf{M}$ to reduce the edge effects without increasing the computation cost.
The  mask matrix $\mathbf{M}$ is defined in the following way:
if the corresponding pixel does not contain the original data, 
the element of  $\mathbf{M}$ is set to zero; otherwise, it is set to one.
The $k$th Landweber iteration is given by
\begin{equation}
	\mathbf{p}_{k+1} =	\mathbf{p}_{k} + \omega
	\left[ \mathbf{A_1}^{\top} (\mathbf{d_1} - 
	\mathbf{M} \mathbf{A_1}	\mathbf{p}_{k}) +
	\mathbf{A_2}^{\top} (\mathbf{d_2} - 
	\mathbf{M} \mathbf{A_2}	\mathbf{p}_{k}) \right]	 \:,
	\label{eq:p_jirigalatu-ebbing2019}
\end{equation}
where $\omega$ is a relaxation factor, $\mathbf{d_1}$ and $\mathbf{d_2}$
are the two gravity gradient components and $\mathbf{A_1}$ and  
$ \mathbf{A_2}$ are the corresponding gravity gradient kernels.
\cite{jirigalatu-ebbing2019} applied their method for processing
two horizontal curvature components of Falcon airborne gravity gradient.

  
\subsubsection{The convolutional equivalent layer with BTTB matrices}

\citeauthor{takahashi2020} (\citeyear{takahashi2020}, 
\citeyear{takahashi2022}) introduced the convolutional equivalent layer for gravimetric and magnetic data processing, respectively.


\cite{takahashi2020} demonstrated that the sensitivity matrix $\mathbf{A}$ (equation \ref{eq:predicted-data-vector}) associated with a planar equivalent layer formed by a set of point masses, each one directly beneath each observation point and considering a regular grid of observation points at a constant height has a symmetric block-Toeplitz Toeplitz-block (BTTB) structure.
A symmetric BTTB matrix has, at least, two attractive properties.
The first one is that it can be defined by using only
the elements forming its first column (or row).
The second attractive property is that any BTTB matrix can be
embedded into a symmetric Block-Circulat Circulant-Block (BCCB) matrix.
This means that the  full sensitivity matrix 
$\mathbf{A}$ (equation \ref{eq:predicted-data-vector})  can be 
completely reconstruct by using the first column of the BCCB matrix only.
In what follows, \cite{takahashi2020} computed the forward modeling 
by using only a single equivalent source. 
Specifically, it is done by calculating  the eigenvalues of the BCCB matrix
that can be efficiently computed by using only the first column of the BCCB matrix via 2D fast Fourier transform (2D FFT).
By comparing with the classic approach in the Fourier domain, 
the convolutional equivalent layer for gravimetric data processing 
proposed by \cite{takahashi2020} performed upward- and downward-continue gravity data with a very small border effects and noise amplification.

By using the original idea of the convolutional equivalent layer 
proposed by \cite{takahashi2020} for gravimetric data processing, \cite{takahashi2022}  developed the convolutional equivalent layer for magnetic data processing.
By assuming a regularly spaced grid of magnetic data  at a constant height 
and a planar equivalent layer of dipoles, \cite{takahashi2022} proved that the sensitivity matrix linked with this layer possess a BTTB structure in the specific scenario where each dipole is exactly beneath each observed magnetic data point. 
\cite{takahashi2022} used a conjugate gradient least-squares (CGLS) algorithm  which 
does not require an inverse matrix or matrix-matrix multiplication. 
Rather, it only requires matrix-vector multiplications per iteration, 
which can be effectively computed using the 2D FFT as a discrete convolution.
The matrix-vector product only uses the elements that constitute the first column of the associated BTTB matrix, resulting in computational time and memory savings.
\citeauthor{takahashi2022} (\citeyear{takahashi2022}) showed the robustness of
the convolutional equivalent layer  in processing magnetic survey that  
violates the requirement of regular grids in the horizontal directions
and flat observation surfaces.

The matrix-vector product in \citeauthor{takahashi2020} (\citeyear{takahashi2020}, \citeyear{takahashi2022}) 
(e.g.,  $\mathbf{d} = \mathbf{A} \mathbf{p}$, such as in equation \ref{eq:predicted-data-vector})
is the main issue to be solved.
To solve it efficiently, these authors involked the auxiliary linear system
\begin{equation}
\mathbf{w} = \mathbf{C} \mathbf{v} \: ,
\label{eq:aux_system_takahashi}
\end{equation}
where $\mathbf{w}$ and $\mathbf{v}$ are, respectively, vectors of data and parameters completed 
by zeros and $\mathbf{C}$ is a BCCB matrix formed by $2Q \times 2Q$ blocks, where each block 
$\mathbf{C}_{q}$, $q = 0, \dots, Q-1$, is a $2P \times 2P$ circulant matrix. 
The first column of $\mathbf{C}$ is obtained by rearranging the first column of
the sensitivity matrix $\mathbf{A}$ (equation \ref{eq:predicted-data-vector}).
Because a BCCB matrix is diagonalized by the 2D unitary discrete Fourier transform (DFT), 
$\mathbf{C}$ can be written as
\begin{equation}
\mathbf{C} = 
\left(\mathbf{F}_{2Q} \otimes \mathbf{F}_{2P} \right)^{\ast} 
\boldsymbol{\Lambda}
\left(\mathbf{F}_{2Q} \otimes \mathbf{F}_{2P} \right) \: ,
\label{eq:C-diagonalized_takahashi}
\end{equation}
where the symbol ``$\otimes$" denotes the Kronecker product \citep{neudecker1969},
$\mathbf{F}_{2Q}$ and $\mathbf{F}_{2P}$ are the $2Q \times 2Q$ and $2P \times 2P$ 
unitary DFT matrices \citep[][ p. 31]{davis1979}, respectively, the superscritpt 
``$\ast$" denotes the complex conjugate and $\boldsymbol{\Lambda}$ is a 
$4QP \times 4QP$ diagonal matrix containing the eigenvalues of $\mathbf{C}$.
Due to the diagonalization of the matrix $\mathbf{C}$, the auxiliary system 
(equation \ref{eq:aux_system_takahashi}) can be rewritten by using equation 
\ref{eq:C-diagonalized_takahashi} and premultiplying both sides of the result 
by $\left(\mathbf{F}_{2Q} \otimes \mathbf{F}_{2P} \right)$, i.e.,
\begin{equation}
\boldsymbol{\Lambda} \left(\mathbf{F}_{2Q} \otimes \mathbf{F}_{2P} \right) 
\mathbf{v} = \left(\mathbf{F}_{2Q} \otimes \mathbf{F}_{2P} \right) 
\mathbf{w} \: .
\label{eq:vec-DFT-system_takahashi}
\end{equation}
By applying the vec-operator \citep{takahashi2020} to both sides of 
equation \ref{eq:vec-DFT-system_takahashi}, by premultiplying  both sides of 
the result by $\mathbf{F}_{2Q}^{\ast}$ and then postmultiplying both sides of the result by 
$\mathbf{F}_{2P}^{\ast}$
\begin{equation}
\mathbf{F}_{2Q}^{\ast} \left[ 
\mathbf{L} \circ \left(\mathbf{F}_{2Q} \, \mathbf{V} \, \mathbf{F}_{2P} \right) 
\right] \mathbf{F}_{2P}^{\ast} = \mathbf{W} \: ,
\label{eq:DFT-system_takahashi}
\end{equation}
where ``$\circ$'' denotes the Hadamard product \citep[][ p. 298]{horn_johnson1991} and 
$\mathbf{L}$, $\mathbf{V}$ and $\mathbf{W}$ are $2Q \times 2P$ matrices obtained 
by rearranging, along their rows, the elements forming the diagonal of matrix 
$\boldsymbol{\Lambda}$, vector $\mathbf{v}$ and vector $\mathbf{w}$, respectively.
The left side of equation \ref{eq:DFT-system_takahashi} contains the 2D 
Inverse Discrete Fourier Transform (IDFT) of the term in brackets, which in turn
represents the Hadamard product of matrix $\mathbf{L}$ and the 2D DFT of matrix 
$\mathbf{V}$.
Matrix $\mathbf{L}$ contains the eigenvalues of $\boldsymbol{\Lambda}$ 
(equation \ref{eq:C-diagonalized_takahashi}) and can be 
efficiently computed by using only the first column of the BCCB matrix 
$\mathbf{C}$ (equation \ref{eq:aux_system_takahashi}).

Actually, in \citeauthor{takahashi2020} (\citeyear{takahashi2020}, \citeyear{takahashi2022}) 
a fast 2D discrete circular convolution \citep{vanloan1992} is used to  process 
very large gravity and magnetic datasets efficiently.
The convolutional equivalent layer was applied to perform upward continuation of large magnetic datasets. 
Compared to the classical Fourier approach, \citeauthor{takahashi2022}'s (\citeyear{takahashi2022}) method produces smaller border effects without using any padding scheme. 

Without taking advantage of the symmetric BTTB structure of the sensitivity matrix (\citeauthor{takahashi2020}, \citeyear{takahashi2020}) 
that arises when gravimetric observations are measured on a 
horizontally regular grid, on a flat surface and considering a regular grid 
of equivalent sources whithin a horizontal layer, \cite{mendoncca2020}  explored the symmetry of the gravity kernel to reduce the number of forward model evaluations.
By exploting the symmetries of the gravity kernels and redundancies in the forward model evaluations on a regular grid and combining the subspace solution based on eigenvectors of the gridded dataset, \cite{mendoncca2020}   
estimated the mass excess or deficiency produced by anomalous sources with positive or negative density contrast. 

\subsubsection{The deconvolutional equivalent layer with BTTB matrices}

To avoid the  iterations of the conjugate gradient method in \cite{takahashi2022}, we can employ 
the deconvolution process.
Equation \ref{eq:DFT-system_takahashi} shows that estimate the  matrix $\mathbf{V}$,
containing the elements of parameter vector $\mathbf{p}$, is a inverse problem that could be 
solved by deconvolution.
From equation \ref{eq:DFT-system_takahashi}, the  matrix $\mathbf{V}$ can be obtain by 
deconvolution, i.e.
\begin{equation}
\mathbf{V} \: = \:  \mathbf{F}_{2Q}^{\ast} \left[ 
\frac{\left(\mathbf{F}_{2Q} \mathbf{W} \, \mathbf{F}_{2P} \right) }{\mathbf{L}}
\right] \mathbf{F}_{2P}^{\ast} \: .
\label{eq:deconvolution_takahashi}
\end{equation}
Equation \ref{eq:deconvolution_takahashi} shows that the parameter vector (in matrix $\mathbf{V}$) 
can be theoretically obtain by dividing each potential-field observations (in matrix $\mathbf{W}$)
by each eigenvalues (in matrix $\mathbf{L}$).
Hence, the parameter vector is constructed by element-by-element division of data by eigenvalues.

However, the deconvolution often is extremely unstable. 
This means that a small change in data can lead to an enormous change in the estimated parameter.
Hence, equation \ref{eq:deconvolution_takahashi} requires regularization to be useful.
We usede wiener deconvolution to obtain a stable solution, i.e.,
\begin{equation}
\mathbf{V} \: = \:  \mathbf{F}_{2Q}^{\ast} \left[ 
\left(\mathbf{F}_{2Q} \mathbf{W} \, \mathbf{F}_{2P} \right)  
\frac{\mathbf{L}^{\ast}}{ \left(\mathbf{L} \: \mathbf{L}^{\ast} + \mu \right)}
\right] \mathbf{F}_{2P}^{\ast} \: ,
\label{eq:wiener_takahashi}
\end{equation}
where the matrix $\mathbf{L}^{\ast}$ contains the complex conjugate eigenvalues and
$\mu$ is a parameter that controls the degree of stabilization. 


\subsection{Solution stability}

The solution stability of the equivalent-layer methods is rarely addressed.
Here, we follow the numerical stability analysis presented in \cite{siqueira-etal2017}.

Let us assume noise-free potential-field data $\mathbf{d}$, 
we estimate a physical-property distribution $\mathbf{p}$ (estimated solution) within the equivalent layer.
Then, the  noise-free data $\mathbf{d}$ are contaminated with additive $D$ different sequences of 
pseudorandom Gaussian noise, creating  different noise-corrupted potential-field data
$\mathbf{d}^\mathbf{o}_\ell$, $\ell = 1, ..., D$.
From each $\mathbf{d}^\mathbf{o}_\ell$, we estimate a physical-property distribution 
$\mathbf{\hat{p}}_\ell$ within the equivalent layer. 

Next, for each noise-corrupted data $\mathbf{d}^\mathbf{o}_\ell$
and estimated solution $\mathbf{\hat{p}}_\ell$, the $\ell$th model perturbation $\delta p_\ell $
and the $\ell$th data perturbation    $\delta d_\ell$ are,  respectively, evaluated by
\begin{equation}
\delta p_\ell = \frac{\parallel \mathbf{\hat{p}}_\ell - {\mathbf{p}} \parallel_2 }
{\parallel {\mathbf{p}} \parallel _2 }, \quad \ell= 1, ..., D,
\label{del_p}
\end{equation}
and
\begin{equation}
\delta d_\ell = \frac{\parallel \mathbf{d}^\mathbf{o}_\ell - \mathbf{d} \parallel _2 }
{\parallel \mathbf{d}\parallel _2}, \quad \ell = 1, ..., D.
\label{del_d}
\end{equation}

Regardless of the particular method used, the following inequality \citep[][ p. 66]{aster2018parameter}  is applicable:
\begin{equation}
\delta p_\ell \leq \kappa \; \delta d_\ell, \quad \ell = 1, ..., D,
\label{condition_number}
\end{equation}
where $\kappa$ is the constant of proportionality between the model perturbation $\delta p_\ell $ 
(equation \ref{del_p}) and the data perturbation   $\delta d_\ell$ (equation \ref{del_d}).
The constant $\kappa$ acts as the condition number of an invertible matrix in a given inversion, and thus
measures the instability of the solution.
The larger (smaller) the value of $\kappa$ the more unstable (stable) is the estimated solution.

Equation \ref{condition_number} shows a linear relationship between the model perturbation and 
the data perturbation.
By plotting $\delta p_\ell$ (equation \ref{del_p}) against $\delta d_\ell$ (equation \ref{del_d}) 
produced by a set of $D$ estimated solution obtained by applying a given equivalent-layer method, 
we obtain a straight line behaviour described by equation \ref{condition_number}.
By applying a linear regression, we obtain a fitted straight line whose estimated slope 
($\kappa$ in equation \ref{condition_number}) quantifies the solution stability.

Here, the  analysis of solution stability is numerically conducted by applying
the classical equivalent-layer technique with zeroth-order Tikhonov regularization,
the convolutional method for gravimetric and magnetic data,
the deconvolutional method (equation \ref{eq:deconvolution_takahashi}) and 
the deconvolutional method with different values for the Wiener stabilization 
(equation \ref{eq:wiener_takahashi}).
